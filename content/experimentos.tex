\chapter{Experimentos y Resultados}
\todo{escribir intro de capitulo}

\section{Colecciones}


Se seleccionan colecciones de datos multi-etiquetas del mundo real que han sido
aplicados previamente en la literatura para evaluar la capacidad predictiva de
los modelos de clasificación \cite{osojnik_multi-label_2017, read_scalable_2012,
	buyukcakir_novel_2018}. La tabla \ref{tab:datasets} enumera sus características
principales, incluyendo métricas que describen su grado de multi-etiquetado (ver
sección \ref{mll_def_formal}). Una descripción detallada de cada una se lista a
continuación:

\begin{description}

	\item{20ng} \footnote{\url{https://www.uco.es/kdis/mllresources/}}: Es una
	      colección que consta de casi 20 mil publicaciones provenientes de grupos de
	      noticias y que abordan 20 tópicos diferentes \cite{lang_newsweeder_1995}.
	      La colección es de texto y fue preprocesada para formar 1006 atributos
	      numéricos.

	\item{Enron}
	      \footnote{\url{http://sourceforge.net/projects/mulan/files/datasets/enron.rar}}:
	      Es una colección de correos electrónicos seleccionados de entre los 500 mil
	      generados por empleados de la compañía eléctrica \textit{Enron} y filtrados
	      durante una investigación por corrupción \cite{hutchison_enron_2004}. Su
	      tamaño, que no supera los 2000 elementos, no es lo suficientemente grande
	      para ser considerado un flujo continuo voluminoso, pero sí cuenta con otras
	      propiedades como la inclusión de fechas y una evolución de los datos en el
	      tiempo \cite{read_scalable_2012}. Las etiquetas se dividen en cuatro
	      grupos, según su género (acuerdos laborales, correos meramente personales,
	      etc); según la información que incluyen, esto es, si el correo contiene
	      enlaces externos, adjuntos, reenvíos, etc; según el tono emocional que
	      reflejan y según el tópico principal que abordan.

	\item{Mediamill}
	      \footnote{\url{https://sourceforge.net/projects/mulan/files/datasets/mediamill.rar}}:
	      Es una colección generada a partir de 80 horas de video provenientes de
	      transmisiones de noticias durante noviembre de 2004
	      \cite{snoek_challenge_2006}. Se seleccionaron más de 43 mil ejemplos y fue
	      manualmente etiquetada con 101 conceptos, que pueden visualizarse en la
	      figura \ref{fig:mediamill}.

\end{description}

\begin{table}[htbp]
	\centering
	\input{tables/datasets.tex}
	\caption{Colecciones multi-etiquetas y sus características. N: número de
		instancias; A: número de atributos; L: número de etiquetas; LC: cardinalidad
		de etiquetas; LD: densidad de etiquetas.}
	\label{tab:datasets}
\end{table}

\begin{figure}
	\includegraphics[width=.9\linewidth]{figures/mediamill.jpg}
	\centering
	\caption{Los 101 conceptos semánticos asociados a la colección
		Mediamill.}
	\label{fig:mediamill}
\end{figure}

Estas son sólo tres de las colecciones usualmente abordadas en la literatura y
se han seleccionado con el objetivo de diversificar el análisis. Enron es una
colección de pocas instancias pero muchas etiquetas, 20ng a la inversa, cuenta
con pocas etiquetas pero muchas instancias; y Mediamill, finalmente, es la
colección con más instancias que hay disponible y cuenta también con un número
relativamente alto de etiquetas.

Durante la ejecución de experimentos, cada colección será convertida a un flujo
sintético. Además, se generará una versión sintética de cada una, siguiendo la
técnica descripta en la sección \ref{generacion_flujos_sinteticos}.

\section{Software}

A continuación se describen las herramientas de software que fueron utilizadas
para la implementación y ejecución de los experimentos.

\begin{description}

	\item[scikit-multiflow]\footnote{\url{https://scikit-multiflow.github.io/}} Es
	      una librería disponible para el lenguaje de programación Python que provee
	      un \textit{framework} para implementar y comparar algoritmos de aprendizaje
	      automático en ambientes de flujos continuos de datos. Incluye pero no se
	      limita a problemas de clasificación multi-etiquetas
	      \cite{montiel_scikit-multiflow_2018}.

	\item[\acrshort{moa}]\footnote{\url{https://moa.cms.waikato.ac.nz/}}
	      \acrfull{moa} es un \textit{framework} para realizar minería de datos sobre
	      flujos continuos de datos, implementada en Java y de código libre.  Incluye
	      algoritmos de evaluación y de aprendizaje automático como clasificadores,
	      regresores, o de \textit{clustering}, pudiendo ser aplicados a problemas de
	      clasificación de etiqueta única o multi-etiquetas.  También incluye
	      herramientas para generar datos sintéticos. Tanto \acrshort{moa} como
	      scikit-multiflow facilitan la reiteración de experimentos con distintas
	      configuraciones, así como la comparación de resultados y la extensión de
	      funcionalidad \cite{bifet_moa_2010}.

	\item[scikit-learn]\footnote{\url{http://scikit-learn.org/stable/index.html}}
	      Es una librería del lenguaje de programación Python que brinda
	      herramientas para realizar evaluación, visualización y análisis de
	      resultados \cite{pedregosa_scikit-learn_2018}.

	\item[Mulan]\footnote{\url{http://mulan.sourceforge.net/index.html}} Es una
	      librería del lenguaje Java especializada en aprendizaje por
	      multi-etiquetas. Mulan incluye una variedad de colecciones de datos
	      multi-etiquetas que han sido la fuente de otros trabajos de la literatura
	      \cite{tsoumakas_mulan_2011}.

\end{description} \todo{nombres de herramientas van en cursiva?}

La herramienta \acrshort{moa} es usada para generar los flujos sintéticos y
provee del marco de trabajo en el cual se implementó el algoritmo de generación
descripto en \ref{generacion_flujos_sinteticos}. Los algoritmos de clasificación
fueron implementados en Python y están disponibles bajo la librería
\textit{scikit-multiflow}. La solución de ensamble \acrshort{efmp} también fue
implementada en esta librería. \textit{scikit-learn}, por su parte, provee la
implementación de las métricas basadas en etiquetas, y las colecciones de datos
fueron extraídas de mulan.

\section{Hardware}

Se ha recibido apoyo del \acrfull{cidetic}, el cual ha proveído de equipos de
altas prestaciones que han proporcionado la capacidad de cómputo necesaria para
llevar a cabo este proyecto. El equipamiento facilitado cuenta con dos nodos de
12 núcleos cada uno, el CPU es un Intel Xeon X5675 de 3.07 GHz de velocidad de
procesamiento, 12 Mb de memoria caché y 6 núcleos. El espacio de almacenamiento
disponible es de 1 Tb y la memoria RAM es de 142 Gb. \todo{Chequear estos datos}

El Sistema Operativo instalado es Ubuntu 18.04 LTS y cuenta con la versión 3.6.9
de Python, el instalador de paquetes Pip en su versión 20.3.3 y Java 1.8.

\section{Algoritmos}
\label{experimentos_algoritmos}

Se realizan los experimentos usando algoritmos multi-etiquetas disponibles en la
librería scikit-multiflow junto con las implementaciones de ensambles
presentadas en este trabajo: \acrfull{efmp} y su variación \acrshort{efmp2}.
Entre los algoritmos del tipo de transformación del problema se seleccionan los
de \acrfull{br}, \acrfull{cc} y \acrfull{mlht}. Tanto \acrshort{br} como
\acrshort{cc} usan \textit{naive} bayes como modelo de clasificación base y
\acrshort{mlht} es ejecutado en su versión basada en \acrfull{lp}, siguiendo los
procedimientos de \citeauthor{read_scalable_2012} \cite{read_scalable_2012}.

En lo que respecta a soluciones de ensamble, los modelos de \acrshort{efmp}
contarán ambos con tres clasificadores base, siendo estos los mencionados en el
párrafo anterior, es decir, \acrshort{cc}, \acrshort{br} y \acrshort{mlht}. La
comparación se hará contra el algoritmo \acrfull{dwm}, tal como ha sido definido
por sus autores \cite{kolter_dynamic_2007} pero adaptado a ambientes de
multi-etiquetas (ver sección \ref{tecnica_algoritmo_ensamble}), y se suman al
análisis los algoritmos de \acrfull{ebr} y \acrfull{ecc}, tal como fueron
definidos por \citeauthor{oza_online_2005} \cite{oza_online_2005} y también han
sido extendidos para soportar problemas de \acrshort{mll}
\cite{read_classifier_2011}. Los tres algoritmos de ensamble extraídos de la
literatura son configurados con diez clasificadores base de \textit{naive}
bayes, para imitar los experimentos conducidos por otros autores de la
literatura \cite{osojnik_multi-label_2017, read_scalable_2012,
	buyukcakir_novel_2018}.

La tabla \ref{tab:algoritmos} es un resumen de los algoritmos seleccionados
junto con los clasificadores base configurados, la referencia bibliográfica y la
clave que será usada en las tablas de resultados.

\begin{table}[htbp]
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\input{tables/algorithms.tex}
	\end{adjustbox}
	\caption{Métodos de clasificación multi-etiquetas seleccionados para ambientes
		de flujos continuos de datos.}
	\label{tab:algoritmos}
\end{table}

\section{Métricas de Evaluación}

En la evaluación de algoritmos de clasificación se usan el conjunto de métricas
que han sido utilizadas en otros trabajos de la literatura, tanto en escenarios
de flujos \cite{sousa_multi-label_2018, zheng_survey_2020,
	osojnik_multi-label_2017} como en \textit{batch} \cite{madjarov_extensive_2012,
	zhang_multi-label_2010, gibaja_tutorial_2015} y fueron descriptas en la
sección \ref{mll_evaluacion}. Estas son:

\begin{description}

	\item[Métricas Basadas en Ejemplos]: \textit{Hamming score}, exactitud del
	      subconjunto, exactitud (ejemplos), precisión (ejemplos), exhaustividad
	      (ejemplos) y medida-f1 (ejemplos).

	\item[Métricas Basadas en Etiquetas]: Micro-exactitud, micro-precisión,
	      micro-exhaustividad, micro-f1, macro-exactitud, macro-precisión,
	      macro-exhaustividad y macro-f1.

	\item[Métricas de Eficiencia]: Velocidad y consumo de memoria.

\end{description}

La medición de velocidad comienza en el momento que inicia la predicción y
entrenamiento del modelo por primera vez y finaliza cuando el clasificador
termina de procesar la última instancia de la colección. Por lo tanto, quedan
afuera las etapas de evaluación, carga de la colección en memoria, generación
del flujo y configuración del entrenamiento. El consumo de memoria también es
monitoreado durante la ejecución del entrenamiento y predicción y toma en cuenta
la estructura completa del modelo y todos sus componentes, incluyendo pesos e
hiper-parámetros propios y de sus clasificadores base.

Los flujos sintéticos son analizados teniendo en cuenta los fenómenos propios de
colecciones del mundo real. A ese fin se estudia el sesgo de etiquetas, la
relación entre etiquetas, la distribución de etiquetas y el espacio de atributos
(ver sección \ref{mll_fenomenos}). \todo{es probable que este análisis se separe
	en una sección aparte y se aborde con mayor profundidad}

\section{Configuración Experimental}

En lo que respecta a modelos de aprendizaje automático, los experimentos fueron
desarrollados en el lenguaje Python usando la librería
\textit{scikit-multiflow}. Los algoritmos de transformación del problema se
aplican tal como han sido implementados en la librería con la salvedad del
\acrshort{mlht}, al que debió introducirle una modificación para manipular la
predicción, se usaba un arreglo disperso para representar las etiquetas
activadas, lo cual producía un desbordamiento de memoria en el entrenamiento de
colecciones grandes como la de mediamill. Se lo suplantó por una estructura de
representación densa. En cuanto a los modelos de ensambles, se adaptaron las
implementaciones existentes de \acrshort{ebr}, \acrshort{ecc} y \acrshort{dwm}
para soportar múltiples etiquetas y para ello se debió modificar la etapa de
combinación de votos para hacer frente a la nueva dimensionalidad de los datos.
Por lo demás, la configuración de los algoritmos es la definida en la sección
\ref{experimentos_algoritmos}.

Para la etapa de evaluación se aplica la técnica de evaluación secuencial
predictiva (\textit{prequential}) con ventanas deslizantes, tal como se
recomienda para ambientes de flujos continuos \cite{gama_evaluating_2013}. Ante
cada ejemplo o ventana de ejemplos arribada el modelo primero realiza la
predicción y luego el entrenamiento. Finalmente las métricas de evaluación son
calculadas una vez procesados todos los ejemplos de la colección y a partir de
todas las predicciones producidas.  Notar que a partir de esta técnica el modelo
predice y entrena todas las instancias, y no solo un subconjunto de ellas como
sucede con la estrategia de \textit{holdout}. La ventana deslizante se configura
en $w = \frac{N}{20}$, es decir, se divide el número total de instancias del
flujo en 20 ventanas, siguiendo las directivas de \textcite{read_scalable_2012}.
Los resultados de la evaluación son agrupados según los tipos de métrica usados,
para facilitar el análisis.

Por otro lado, los flujo de datos sintéticos fueron generados a partir de las
colecciones 20ng, enron y mediamill, y cada una cuenta con cien mil instancias.
Sus atributos se generan usando el algoritmo \acrfull{rbf} (ver sección
\ref{stream_syn}).

\section{Resultados}

\subsection{Flujos Continuos Sintéticos}

\todo{Completar valores faltantes}
\begin{table}[htbp]
	\centering
	\input{tables/syn/stats.tex}
	%\begin{adjustbox}{max width=\textwidth}
	%\input{tables/syn/stats.tex}
	%\end{adjustbox}
	\caption{Características de las colecciones sintéticas generadas.
		N: número de instancias; L: número de etiquetas; LC: cardinalidad de
		etiquetas; LD: densidad de etiquetas.}
	\label{tab:syn_datasets}
\end{table}

\subsection{Clasificaciones}

La metodología propuesta permitió evaluar los diferentes algoritmos de
clasificación multi-etiqueta para los diferentes \textit{streams} utilizando las
configuraciones sin ensambles, los ensambles de referencias y los métodos de
ensamble propuestos. Los resultados se dividen en métricas de ajustes del modelo
basadas en ejemplos (tabla~\ref{tab:example_based}), métricas basadas en
etiquetas (tabla \ref{tab:label_based}), y por último las métricas de eficiencia
(tabla \ref{tab:efficiency}) que cuantifican el tiempo de procesamiento y
espacio de almacenamiento de los modelos. Se marca en negrita la celda
correspondiente al modelo que obtuvo el mejor valor de métrica para la
correspondiente colección de datos. Para cerrar la sección se hace una
comparativa contra experimentos de la literatura de referencia.

\subsubsection{Resultados para Métricas Basadas en Ejemplos}

\begin{table}[htbp]
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\input{tables/evaluations/example_based_1.tex}
	\end{adjustbox}
	\begin{adjustbox}{max width=\textwidth}
		\input{tables/evaluations/example_based_2.tex}
	\end{adjustbox}
	\caption{Resultados de métricas basadas en ejemplos sobre los
		\textit{streams} seleccionados para cada algoritmo evaluado.}
	\label{tab:example_based}
\end{table}

Los valores de F1 obtenidos para la evaluación basada en ejemplos (Tabla
\ref{tab:example_based}) muestra que \acrshort{efmp} y \acrshort{efmp2} fueron
mejores que los \textit{baselines} de ensambles en todos los casos.  Además,
superó a los que no utilizan ensambles en el \textit{stream} de Enron.  En los
casos de 20ng, \acrshort{efmp} fue superado en un 0.001\% por \acrshort{br} y en
Mediamill \acrshort{mlht} superó a \acrshort{efmp} en un 0.015\%. Para el caso
de \textit{Exact-match} el modelo dominante es \acrshort{mlht}, lo cual es un
resultado en consonancia con otros estudios de la literatura, y los modelos
propuestos se ubican en segundo lugar para dos de las tres colecciones. En
cuanto al \textit{Hamming score} los resultados son muy similares entre
colecciones de datos, con los modelos de \acrshort{dwm} sacando una leve ventaja
para 20ng y Enron pero siendo superado por \acrshort{efmp} y \acrshort{mlht}
para Mediamill.

\subsubsection{Resultados para Métricas Basadas en Etiquetas}

\begin{table}[htbp]
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\input{tables/evaluations/label_based_macro.tex}
	\end{adjustbox}
	\begin{adjustbox}{max width=\textwidth}
		\input{tables/evaluations/label_based_micro.tex}
	\end{adjustbox}
	\caption{Resultados de métricas basadas en etiquetas sobre los
		\textit{streams} seleccionados para cada algoritmo evaluado.}
	\label{tab:label_based}
\end{table}

La métrica de \textit{F-score macro} muestra resultados favorecedores para los
modelos presentados. \acrshort{efmp2} obtuvo un valor superior para Mediamill y
\acrshort{efmp} fue el mejor para 20ng y el segundo mejor para Enron, por
centésimas de diferencia con respecto al modelo \acrshort{br}. En lo que
respecta a la comparativa entre soluciones de ensambles, \acrshort{efmp} y
\acrshort{efmp2} superan a las demás para todas las colecciones evaluadas.  En
relación a los modelos de \acrshort{dwm}, estos muestran una disparidad entre
los valores de precisión y \textit{recall}. Vease por ejemplo el caso de
\acrshort{dwm} (\acrshort{cc}) para 20ng, donde logra más de un 80\% de
precisión (mejor clasificador) pero un \textit{recall} por debajo del 20\% (segundo peor
clasificador). Mismo caso pero a la inversa con \acrshort{dwm} (\acrshort{br})
para Mediamill, el cual consigue un 40\% de \textit{recall} (tercer mejor
clasificador) pero apenas un 0.006\% de precisión (tercer peor clasificador).
Esta imparidad logra suavizarse en los modelos de \acrshort{efmp} presentados y
se ve reflejado en mejores valores de \textit{f-score}.

También son favorecedores los valores de las métricas de \textit{F-score micro}
para los modelos presentados. \acrshort{efmp} es el mejor para las colecciones
20ng y Enron y es apenas superado por \acrshort{mlht} para el \textit{stream} de
Mediamill. En la comparativa de métodos de ensambles, \acrshort{efmp} es el
mejor modelo para todos las métricas a excepción del caso de \textit{recall}
para Mediamill (donde \acrshort{ebr} obtiene el mejor valor), y los casos de
precisión para Enron y 20ng, donde \acrshort{dwm} (\acrshort{cc}) logra una
clara diferencia. Al respecto de este ultimo modelo se puede hacer las mismas
consideraciones en cuanto a la disparidad entre precisión y \textit{recall}.

\subsubsection{Resultados para Métricas de Eficiencia}

\begin{table}[htbp]
	\centering
	\begin{adjustbox}{max width=\textwidth}
		\input{tables/evaluations/efficiency.tex}
	\end{adjustbox}
	\caption{Resultados de métricas de eficiencia sobre los
		\textit{streams} seleccionados para cada algoritmo evaluado.}
	\label{tab:efficiency}
\end{table}

Tal como es de esperar, la tabla~\ref{tab:efficiency} muestra que los modelos
\textit{baselines} que no son soluciones de ensambles hacen un uso de espacio
significativamente menor que los modelos de ensambles y logran tiempos de
ejecución menores. Sin embargo, en la comparativa entre ensambles, los
algoritmos propuestos (\acrshort{efmp} y \acrshort{efmp2}) reducen tanto el
espacio de almacenamiento como el tiempo de procesamiento de los
\textit{streams} de Enron y 20ng. Las métricas de tiempo se reducen
significativamente para 20NG y Enron, mientras que en Mediamill \acrshort{ebr}
hace un uso significativamente menor de tiempo que los ensambles presentados.

\subsubsection{Comparativa contra Literatura de Referencia}

A partir de los resultados obtenidos se realiza una comparativa contra otros
estudios del campo. A este fin se seleccionaron los trabajos de
\citeauthor{osojnik_multi-label_2017}
(\citeyear{osojnik_multi-label_2017})~\cite{osojnik_multi-label_2017},
\citeauthor{roseberry_multi-label_2018}
(\citeyear{roseberry_multi-label_2018})~\cite{roseberry_multi-label_2018},
\citeauthor{buyukcakir_novel_2018}
(\citeyear{buyukcakir_novel_2018})~\cite{buyukcakir_novel_2018} y
\citeauthor{sousa_multi-label_2018}
(\citeyear{sousa_multi-label_2018})~\cite{sousa_multi-label_2018}. Si bien las
métricas y colecciones utilizadas varian según el estudio, todos los trabajos
parten de una configuración experimental similar a la de este trabajo y usan el
método \textit{prequential} para evaluar rendimientos.

\citeauthor{osojnik_multi-label_2017} presentaron experimentos sobre las
colecciones de 20ng y Enron bajo métricas basadas en ejemplos (\textit{Hamming
	score}, \textit{F1} y \textit{Exact-match}), métricas basadas en etiquetas
(Precisión, \textit{Recall} y \textit{F1}, todas ellas con promedio micro y
macro). Comenzando por las métricas basadas en ejemplos, el modelo
\textit{iSOUP-MT} es el que mejor \textit{Hamming score} obtiene con un valor de
0.9523 y es levemente superado por \acrshort{efmp} (0.954) y \acrshort{efmp2}
(0.955). Para el caso de Enron el ganador es \textit{iSOUP-MT} (en su versión de
ensambles) con un valor de 0.942 y supera a \acrshort{efmp2} (0.936). En cuanto
al \textit{Exact-match} nuestras soluciones superan a la mejor solución de los
autores, iSOUP-RT (0.117), en un 2.05\% para 20ng y son superadas en un 6.25\%
por el modelo iSOUP-MT para Enron (0.244). En lo que respecta al \textit{F1},
\acrshort{efmp} supera en un 2.8\% a iSOUP-RT (0.118) para 20ng y en poco más de
un 1\% a iSOUP-MT para Enron (0.329).

Con respecto a las métricas basadas en etiquetas los resultados también
favorecen a los métodos aquí presentados. Para el stream 20ng \acrshort{efmp2}
supera en un 25.5\% a iSOUP-RT bajo la métrica de precisión macro, en un 238\%
en \textit{recall} macro a iSOUP-MT y en un 142\% en \textit{f1} a ese mismo
modelo. Para Enron nuestros modelos superan en un 111\%, 156\% y 164\% a
iSOUP-RT para las mismas medidas mencionadas, respectivamente. Las métricas de
promedio micro también favorecen a nuestros modelos. Para 20ng \acrshort{efmp}
es superado en un 41.1\% por iSOUP-MT (en versión ensamble) en precisión, supera
en un 213.6\% a iSOUP-MT en \textit{recall} y en un 125\% en \textit{f1}. Para
Enron es superado en 33.3\% por iSOUP-MT (en versión ensamble) en precisión,
supera en un 45.5\% a iSOUP-RT en \textit{recall} y en un 10.8\% en \textit{f1}.

\citeauthor{sousa_multi-label_2018} han presentado experimentos sobre los tres
\textit{streams} y bajo métricas basadas en ejemplos, en particular las de
\textit{accuracy}, \textit{exact-match}, precisión, \textit{recall} y
\textit{f1}. Para la comparativa se toma el mejor modelo de los autores para
cada métrica. Comenzando por la métrica de \textit{exact-match} los autores han
logrado mejores resultados. Nuestros modelos son superados en un 50.6\% para
20ng, en un 71.7\% para Enron y en un 75.5\% para Mediamill. También para la
precisión, recall y \textit{f1} \citeauthor{sousa_multi-label_2018} han logrado
mejores resultados en general.  En precisión logran superar en un 42.5\% a
\acrshort{efmp} para 20ng, en un 25.6\% a \acrshort{efmp2} para Enron y son
superados en un 0.5\% por \acrshort{efmp} para Mediamill. En \textit{recall}
superan en un 27.5\% a nuestros modelos para 20ng, en un 29\% para Enron y en un
7\% para Mediamill.  Finalmente, en \textit{f1} superan en un 37\% a nuestros
modelos para 20ng, en un 23\% para Enron y en un 15\% para Mediamill. Los
autores no presentan resultados bajo métricas basadas en etiquetas.

\citeauthor{roseberry_multi-label_2018}, por su parte, diseñaron el modelo
ML-SAM-kNN y lo pusieron a prueba con las tres colecciones y las métricas de
\textit{exact-match} y \textit{f1}. En la comparativa obtuvimos que
\textit{efmp} es superado en un 26\% para 20ng, en un 86\% para Enron y en un
92\% para Mediamill. No obstante, bajo la métrica de \textit{f1}, nuestros
modelos superan en un 65\% y 229\% a sus modelos para 20ng y Enron
respectivamente y es superado en un 26\% para Mediamill.

Finalmente, \citeauthor{buyukcakir_novel_2018} presentaron el modelo de
ensambles \textit{GOOWE-ML} bajo las métricas de \textit{hamming score},
\textit{accuracy} basado en ejemplos, \textit{f1} basado en ejemplos y
\textit{f1} micro, basado en etiquetas. Si bien realizaron pruebas sobre varios
\textit{streams} el único en común con este trabajo es el de 20ng. Dicho esto,
su modelo consigue mejores valores para las métricas de \textit{f1} micro,
\textit{accuracy}, y \textit{f1} basado en ejemplos (13\%, 24\% y 26\% de
mejora, respectivamente) y peores valores para la métricas de \textit{hamming
	score} donde nuestros modelos lo superan en un 0.3\%.

En resumen, el resultado de los métodos propuestos muestra que son competitivos
respecto a la literatura de referencia. En particular, para las pruebas
realizadas con el conjunto de datos 20NG los valores de F-Score basado en
ejemplos obtenidos superan a~\cite{osojnik_multi-label_2017} pero no son mejores
que otros como~\cite{sousa_multi-label_2018, buyukcakir_novel_2018,
	roseberry_multi-label_2018}.  En las pruebas realizadas con Enron, los métodos
propuestos superan a~\cite{osojnik_multi-label_2017}, duplican el rendimiento
de~\cite{roseberry_multi-label_2018} y son superados
por~\cite{sousa_multi-label_2018}. Finalmente, para el conjunto de datos
Mediamill tanto~\cite{sousa_multi-label_2018}
como~\cite{roseberry_multi-label_2018} superan nuestra propuesta.


