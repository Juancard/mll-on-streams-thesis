\chapter{Preliminares}


En este capítulo se presenta el marco teórico de este trabajo, dando un panorama
general de cada una de las disciplinas abordadas e introduciendo los conceptos
básicos y fundamentales para entender el proyecto. Se comienza con la definición
de la taxonomía del campo de estudio, luego \todo{describir las siguientes
	secciones}

\section{Taxonomía del Campo de Estudio}

\begin{figure}
	\includegraphics[width=.9\linewidth]{figures/study_field_taxonomy_v2.png}
	\centering
	\caption{Taxonomía del campo de estudio.}
	\label{fig:campo_estudio}
\end{figure}

En pocas palabras, el presente trabajo de investigación se enmarca en las áreas
de \textit{big data} y minería de datos, con aplicación en escenarios de
\textit{streaming} o flujos continuos de datos y abordando clasificaciones
multi-etiquetas.

La figura~\ref{fig:campo_estudio} es un esquema que ilustra la taxonomía del
campo de estudio y la interrelación entre las áreas de investigación
involucradas.

\section{Aprendizaje Automático}

El aprendizaje automático, también conocido por su término en inglés
\comillas{\textit{Machine Learning}}, se enmarca dentro del área de la
\acrfull{ia} y estudia cómo las computadoras pueden \comillas{aprender} o
mejorar su rendimiento meramente a partir de datos y sin la intervención de un
ser humano.  La idea detrás de esta disciplina es lograr reconocer patrones
subyacentes en los datos y tomar decisiones basándose en ellos. Por ejemplo, un
problema de aprendizaje automático es el de reconocer dígitos escritos a mano a
partir de un conjunto de ejemplos (ver figura~\ref{fig:reconocimiento_digitos}).
Aquí se tienen un conjunto de imágenes, cada una representando un dígito del 0
al 9, y el objetivo es construir un modelo que sea capaz de detectar de qué
dígito se trata. Otro ejemplo es el de hallar documentos de texto que son
relevantes a una consulta del usuario. En este caso el modelo recibe un conjunto
acotado de términos, los cuales describen una necesidad de información del
usuario, y el modelo debe ser capaz de retornar los documentos que satisfacen la
consulta.

Estos problemas se suelen categorizar en aprendizaje supervisado o no
supervisado, de acuerdo a si se conoce o no de antemano el concepto o etiqueta
que define a los datos. Se desarrollará más sobre este punto en las próximas
secciones \todo{expandir la definición de supervisada vs no supervisada}. De
entre los problemas de aprendizaje supervisado se destaca aquí el de
clasificación, el cual será descrito a continuación.

\begin{figure}
	\includegraphics[width=0.66\linewidth]{figures/digits_recognition_v2.png}
	\centering
	\caption[Dígitos escritos a mano.]{Dígitos escritos a mano. Fuente: \citetitle{hastie_elements_2009}
		(\citeyear{hastie_elements_2009}).}
	\label{fig:reconocimiento_digitos}
\end{figure}

\section{Clasificación}

\subsection{Definición}
\label{clasificacion}

La clasificación es una tarea de minería de datos muy popular que consiste en
hallar modelos que describen la o las clases intrínsecas de los datos. La clase
corresponde a un concepto que representa al dato y es una etiqueta categórica,
es decir, un valor discreto de entre un conjunto de valores previamente
conocidos. Estos modelos, también llamados clasificadores, son capaces de
predecir la clase a la que corresponden datos previamente desconocidos. Por
ejemplo, se puede construir un modelo de clasificación para categorizar nuevos
correos electrónicos  de acuerdo a si se trata de correo basura (también
conocido como \comillas{\textit{spam}}) o no. Dicho análisis puede ayudar a
obtener un mayor entendimiento de los datos a alto nivel. Las tareas de
clasificación han sido aplicadas en áreas tales como las de aprendizaje
automático, reconocimiento de patrones o estadística.

En un principio, buena parte de los algoritmos se ejecutaban en memoria, con la
limitación de espacio de almacenamiento que eso conlleva. Investigaciones más
recientes han desarrollado técnicas para escalar los algoritmos de tal manera
que puedan manejar datos de mayor tamaño, alojados en memoria, en disco o
procesados bajo demanda. Las aplicaciones para este tipo de tareas son numerosas
y entre ellas se encuentran las de detectar fraudes o realizar diagnósticos
médicos, entre otras.

La clasificación de datos consta de dos etapas, una de aprendizaje y otra de
clasificación o predicción. Durante la tarea de aprendizaje se construye el
modelo de clasificación  el cual describe un determinado número de clases o
conceptos. También se conoce esta etapa como la de entrenamiento, ya que se
selecciona un subconjunto de los datos, llamado conjunto de entrenamiento, que
consta de instancias o tuplas seleccionadas aleatoriamente y con una o más
etiquetas asociadas. Formalmente, el problema de clasificación puede ser
formulado de la siguiente manera. Se recibe un conjunto etiquetado de
instancias, tupas o ejemplos de la forma $( X, y )$ donde cada tupla es un
vector $X=(x_{1},x_{2},\dots,x_{n})$, siendo cada valor una característica
distintiva, atributo o \textit{feature} de la instancia. El vector $y$ por su
parte toma un valor de entre $n$ clases diferentes.

Este tipo de tareas se engloban dentro del campo de aprendizaje supervisado ya
que para cada instancia la etiqueta es conocida de antemano, y es aprovechada
para guiar o, siguiendo la metáfora, \comillas{supervisar} el aprendizaje del
clasificador. Esta es la diferencia principal contra algoritmos de aprendizaje
no supervisado, en los cuales la etiqueta no es conocida y se deben aplicar
técnicas para salvar esta restricción.

La primera etapa de una clasificación puede ser vista también como el
aprendizaje de una función $y=f(X)$ que pueda predecir la clase $y$ para una
tupla $X$. Por ejemplo, $X$ podría ser un mensaje de correo y la etiqueta $y$ la
decisión de si se trata de un correo basura o no. Desde esta perspectiva
queremos aprender una función que sea capaz de distinguir las clases
subyacentes.  Usualmente, esta asociación es llevada a cabo por algoritmos de
aprendizaje, los cuales internamente usan funciones matemáticas o reglas de
decisión que les permiten procesar los atributos de entrada y generar una salida
acorde. Algunos ejemplos de este tipo de algoritmos son los árboles de decisión,
\textit{naive} bayes, perceptrón, entre otros. Más adelante se retomará sobre
este punto para describir en detalle algunos algoritmos representativos del
campo.

En la segunda etapa el modelo es usado para clasificar y realizar predicciones
sobre datos desconocidos. A este fin, se calcula un valor que refleja la calidad
del clasificador y es denominado \comillas{métrica de evaluación}. Una de ellas
es la exactitud o \textit{accuracy}, pero no es la única.  Durante la etapa de
entrenamiento esta estimación puede ser imprecisa, tomando un valor que tiende a
ser \comillas{optimista} o que da un valor de exactitud mayor al rendimiento
real.  Esto sucede porque el clasificador puede llegar a incorporar anomalías
particulares en el conjunto de datos de entrenamiento, las cuales no tienen que
ver tanto con el dominio de aplicación en el cual se enmarca la tarea, sino más
bien con \comillas{ruido}, datos erróneos o simplemente instancias que no
reflejan correctamente los objetos del mundo real. Este fenómeno es llamado
\comillas{sobreajuste} u \comillas{\textit{overfit}} y se han diseñado técnicas
para reducirlo. Una de ellas consiste en separar de entre los datos de la
colección completa, un subconjunto conocido como \comillas{conjunto de prueba} o
de \textit{testing} que no se usa durante el entrenamiento y a partir del cual
se realizan predicciones y se calculan las métricas de evaluación.

Así pues, la tarea de evaluación es fundamental, ya que es la vía a partir de la
cual se determina qué algoritmos o técnicas son más apropiados que otros para un
problema en particular. Asimismo, provee la información necesaria para corregir
o ajustar los parámetros de los algoritmos y así obtener modelos más robustos.

En definitiva, las etapas de aprendizaje y predicción se aplican
consecutivamente con el objetivo de lograr generar un clasificador capaz de
predecir con éxito las etiquetas de instancias nuevas y a priori desconocidas
por el modelo.

\subsection{Algoritmos}
\label{clasificacion_algoritmos}

Como se mencionó en la sección anterior, una de las etapas de la clasificación
consiste en generar un modelo capaz de clasificar instancias no observadas. En
esta etapa de aprendizaje, se pueden aplicar diversos tipos de algoritmos de
clasificación de acuerdo a la naturaleza de la tarea en particular que se desea
abordar. A continuación, se describen algunos de estos algoritmos en detalle a
fin de ahondar sobre el concepto de clasificación en el aprendizaje automático.
Además, más adelante estos algoritmos serán particularmente relevantes para el
desarrollo del presente trabajo de investigación.

\subsubsection{\textit{Naive} Bayes}

\textit{Naive} Bayes es uno de los algoritmos que pertenecen a la familia de
clasificadores probabilísticos y se destaca por ser computacionalmente simple,
interpretación, y al mismo tiempo brinda un  rendimiento competitivo en
comparación con otros modelos más complejos. Se dice que es un clasificador
estadístico porque se basa en el teorema de Bayes. La idea es computar una
probabilidad para cada una de las clases, basada en los atributos de la
instancia y seleccionar aquella de mayor probabilidad. El término
\comillas{\textit{naive}} es el inglés para el término
\comillas{\textit{ingenuo}} y nace de la presunción que hace el algoritmo de que
los atributos son independientes entre sí, o condicionalmente independientes.
Esta presunción raramente se cumple en los escenarios donde se aplica, pero
contribuye a su simplicidad computacional y a su velocidad durante el
entrenamiento.

Para entender cómo funciona este algoritmo, es bueno abordar primero  el teorema
de Bayes. Formalmente, se define de la siguiente manera:

\begin{equation}
	P(H \mid X) = \frac{P(X \mid H) P(H)}{P(X)}
\end{equation}

En esta ecuación, el vector $X$ es una tupla definida tal como en la sección
anterior y en términos bayesianos representa la \comillas{evidencia}. $P(X)$,
por lo tanto, es la probabilidad de que la tupla contenga los atributos que
efectivamente posee. Por su parte, $H$ es la hipótesis de que la tupla pertenece
a una determinada clase y $P(H)$ su probabilidad. Esta es conocida como
probabilidad \comillas{a priori}. De la misma manera, $P(H|X)$ es la
probabilidad de que la hipótesis $H$ sea cierta bajo la evidencia $X$. A esta se
la llama probabilidad \comillas{a posteriori} con $H$ condicionada por $X$ y es
el valor que se quiere determinar en una tarea de clasificación.  Finalmente,
$P(X|H)$ indica la probabilidad de que la tupla tenga unos atributos
determinados dado que se satisface la hipótesis.

A partir de dicha definición, y de forma similar, se expresa la ecuación de
\textit{Naive} Bayes de la siguiente manera:

\begin{equation}
	P(C_{i} \mid X) = P(X \mid C_{i}) P(C_{i})
\end{equation}

Aquí el término $P(X)$ es descartado porque se asume constante para todas las
clases. La hipótesis $H$ es representada como $C_{i}$ que es un valor de la
tupla $C=(C_{1},C_{2},\dots,C_{q})$, donde $q$ es el número de clases. La
presunción \comillas{ingenua} es aplicada para el cálculo del término $P(X \mid
	C_{i})$ gracias a lo cual se puede definir de la siguiente manera:

\begin{equation}
	P(X \mid C_{i}) = \prod\limits_{k=1}^n{P(x_{k} \mid C_{i})} =
	P(x_{1} \mid C_{i}) \times
	P(x_{2} \mid C_{i}) \times \dots, \times
	P(x_{n} \mid C_{i})
\end{equation}

Finalmente, el modelo seleccionará la clase que maximice el valor de
probabilidad y esa será la salida final del algoritmo.

Como se ha dicho anteriormente, la simplicidad, velocidad computacional y su
competitividad en métricas de exactitud hacen de \textit{Naive} Bayes un
algoritmo destacado en el campo de aprendizaje automático
\cite{wickramasinghe_naive_2020} y ha sido aplicado para problemas diversos,
tales como el de hallar errores en programas de computación
\cite{arar_feature_2017}, predecir enfermedades del corazón
\cite{dulhare_prediction_2018} o detectar ataques en una red de computadoras
\cite{kalutarage_detecting_2015}.


\subsubsection{Árboles de Decisión}

Árboles de decisión es un modelo de clasificación que se destaca por ser de
fácil interpretación e intuitivo para el ser humano. De hecho, se puede generar
una representación gráfica del árbol generado para asistir a la comprensión del
modelo y así entender a más alto nivel cómo se comporta durante una predicción.
En cuanto a su estructura, un árbol de decisión contiene nodos, cada uno
representando un atributo de la colección. Estos nodos se conectan con otros
nodos a partir de enlaces o \comillas{ramas} que representan un valor o un rango
de valores de ese atributo.  Los nodos de menor jerarquía son llamados
\comillas{hojas} y contienen la clase de la predicción, y el nodo de mayor
jerarquía es llamado \comillas{raíz}. Al momento de predecir una instancia
nueva, la clasificación se realiza de la siguiente manera:  se toma la instancia
nueva, la cual no tiene una etiqueta asociada, y los valores de sus atributos
son comparados contra los del árbol, luego se traza un camino desde el nodo raíz
hasta la hoja. Finalmente, la clase que contiene la hoja es seleccionada y será
parte de la predicción resultante. \todo{Figura de un árbol}

Los árboles de decisión se generan a partir de un algoritmo de inducción.
Existen varios de estos algoritmos, pero todos son variantes que han sido
diseñadas bajo un mismo principio: construir  el árbol de una manera
\comillas{voraz}\footnote{Se le llama voraz o \textit{greedy} a un algoritmo que
	busca hallar la opción óptima en cada paso y, de esta manera, alcanzar la
	solución general óptima para resolver un problema.  Esto lo diferencia de
	algoritmos como los de \textit{backtracking}, los cuales exploran distintas
	posibilidades y pueden volver al inicio en búsqueda de una mejor solución.},
comenzando desde el nodo raíz (conocido como enfoque \textit{top-down}) y
eligiendo en cada paso el atributo más informativo o que maximice alguna medida
de ganancia de información. Entre estos algoritmos de inducción vale destacar
los siguientes:

\begin{description}

	\item[\acrshort{id3}] Son las siglas de \comillas{\textit{\acrlong{id3}}} y
	      fue desarrollado en 1986 por Ross Quinlan. Consiste en crear un árbol de
	      múltiples vías, buscando para cada nodo el atributo categórico que lance
	      la mayor ganancia de información para las clases categóricas. Los árboles
	      crecen en un tamaño máximo y luego se realiza el paso de poda para mejorar
	      el poder de generalización del modelo sobre datos desconocidos.

	\item[C4.5] Es la evolución del algoritmo \acrshort{id3}. La principal mejora
	      con respecto a su predecesor es que elimina la restricción de que los
	      atributos deban ser categóricos. Esto lo consigue particionando el valor
	      continuo en rangos o en un conjunto de intervalos discretos. A su vez,
	      C4.5 convierte el árbol entrenado en conjuntos de reglas de decisión.

	\item[\acrshort{cart}] Son las siglas de \comillas{\textit{\acrlong{cart}}} y
	      es un algoritmo muy similar al C4.5, pero que soporta clases numéricas, lo
	      cual permite que este algoritmo pueda ser utilizado para resolver
	      problemas de regresión.

\end{description}

Una tarea fundamental en la generación de un árbol es definir un criterio de
división. El objetivo del criterio de división es seleccionar el mejor atributo
en cada paso y existen diversas técnicas para abordar el problema. Una de ellas
es la de \comillas{Ganancia de Información}, usada por el algoritmo
\acrshort{id3}. La técnica de ganancia de información busca seleccionar el
atributo que posee mayor variabilidad o representatividad de los datos y se
sustenta en el cálculo de la entropía o medida de desorden. La idea de fondo es
hallar el atributo que reduzca la entropía esperada. La entropía en el conjunto
de datos $D$ se calcula de la siguiente manera:

\begin{equation}
	Entropia(D) = - \sum_{i=1}^{q} p_{i}\log_{2}(p_{i})
\end{equation}

Nótese que $p_{i}$ corresponde a la probabilidad de que una tupla de $D$
corresponda a la clase $C_{i}$.  A partir de la entropía, se define la ganancia
de información como:

\begin{equation}
	Ganancia(A) = Entropia(D)
	- \sum_{j=1}^{v} \frac{\left\| D_{j} \right\|}{\left\| D \right\|}
	\times Entropia(D_{j})
\end{equation}

Aquí el atributo $A$ divide al conjunto de datos en $v$ particiones, siendo $v$
los valores posibles que toma $A$. $D_{j}$ es el subconjunto de los datos cuyas
tuplas poseen el valor $v$ del atributo $A$, siendo $\left\|D_{j}\right\|$ su
cardinalidad o número de instancias del subconjunto. Al dividir este término por
la cardinalidad del conjunto de datos, se obtiene un valor que representa el
peso de la partición y que es aplicado sobre la entropía esperada. Este proceso
se repite para todos los atributos y, una vez obtenidos los valores de ganancia
para cada uno de ellos, se elige aquel que maximiza la ganancia y finalmente el
atributo seleccionado será el criterio de separación en el nodo.

El algoritmo C4.5 introdujo una mejora en esta técnica llamada \comillas{Razón
	de Ganancia}. La misma busca disminuir uno de los efectos adversos que provoca
la técnica de ganancia de información, esta es, que tiende a favorecer a
atributos con un mayor número de valores posibles. La razón de ganancia, en
primer lugar, reemplaza la fórmula $Entropia(D)$ por la siguiente expresión:

\begin{equation}
	EntropiaRG_{A}(D) = - \sum_{j=1}^{v} \frac{\left\| D_{j} \right\|}{\left\| D \right\|}
	\times \log_{2}(\frac{\left\| D_{j} \right\|}{\left\| D \right\|})
\end{equation}

A partir de allí, el cálculo de la razón de ganancia hace uso de la ganancia y
de la entropía y se formula como:

\begin{equation} \label{eq:gan_c45}
	RazonGanancia(A) = \frac{Ganancia(A)}{EntropiaRG_{A}(D)}
\end{equation}

Finalmente, el atributo de mayor razón de ganancia es seleccionado como criterio
de corte y se continúa el cálculo con los siguientes subnodos.
\todo{Aplicaciones de árboles de decisión?}

\todo[inline]{Subsección para sgd?, svm?, perceptrones?}

\subsubsection{Ensambles} \todo{Figura de ensamble}

Los ensambles son un conjunto de clasificadores cuyas salidas son combinadas
entre sí con el objetivo de realizar mejores predicciones que cualquiera de
ellos individualmente. En pocas palabras, el enfoque de ensambles consiste en
generar $k$ clasificadores llamados \comillas{clasificadores base}, desde un
mismo algoritmo o no, y entrenarlos con distintos subconjuntos de la colección
de entrenamiento original. Dada una tupla nueva, cada clasificador devuelve su
propia predicción, llamada \comillas{voto}, y luego el ensamble combina cada uno
de estos votos siguiendo algún método de combinación elegido, de forma tal de
producir una predicción final óptima.

La aplicación de ensambles en problemas de clasificación nace de la
imposibilidad de generar un único modelo capaz de generalizar lo suficiente como
para lograr un rendimiento perfecto. Ante la presencia de datos ruidosos,
atípicos o erróneos los clasificadores pueden tender a clasificar mejor para un
subconjunto de datos y no tan bien para otros. Este escenario es aprovechado por
el enfoque de ensambles, ya que su éxito tiene correlación directa con la
existencia de \comillas{diversidad} en la clasificación, distinguiendo el
concepto de diversidad como la existencia de variabilidad entre los modelos,
entre hiperparámetros o entre particiones del conjunto de datos. En definitiva
se entiende que, cuanto mayor es esta diversidad, mayor es la probabilidad de
aislar los posibles errores particulares de un modelo, y al suceder esto, el
error terminará siendo filtrado por el ensamble en la clasificación final. En
consecuencia, se espera lograr una disminución del error total de la
clasificación así como también una mayor exactitud en la predicción, comparando
contra la salida individual de cada clasificador base. Sumado a esto, un
enfoque de ensambles abre la posibilidad de distribuir y/o paralelizar el
cómputo de la predicción, pudiendo así mejorar los tiempos de ejecución durante
el entrenamiento.

En suma, existen distintos tipos de ensamble de acuerdo a su construcción y
arquitectura. A continuación se describen tres de ellos: los ensambles de tipo
\comillas{\textit{bagging}}, los de tipo \comillas{\textit{boosting}} y los de
tipo \comillas{\textit{stacked}}.

\begin{description}

	\item[Bagging] Esta es una de las primeras técnicas de ensambles conocidas y
	      fue introducida por
	      \citeauthor{breiman_bagging_1996}\cite{breiman_bagging_1996}. La misma se
	      desarrolla de la siguiente manera: dado un conjunto de entrenamiento $D$
	      con $n$  tuplas, \textit{bagging} genera un número $m$ de nuevos conjuntos
	      de datos de entrenamiento, cada uno con $n$ tuplas. Para esto se toman
	      tuplas del conjunto original de manera aleatoria y con reemplazo, es decir
	      que puede haber tuplas repetidas y otras que no están incluidas en el
	      nuevo conjunto.  Luego a partir de cada conjunto nuevo, se entrena un
	      clasificador $M_{i}$. Cada clasificador puede ser del mismo tipo porque la
	      diversidad está dada por los datos. En la etapa de clasificación, cada
	      modelo $M_{i}$ genera una predicción que cuenta como un voto. El ensamble
	      cuenta los votos y elige la clase con mayor cantidad de votos, siendo esta
	      la decisión final del ensamble.


	\item[Boosting] En la técnica de \textit{boosting} se asigna un peso a cada
	      tupla de entrenamiento y se generan un conjunto de clasificadores, cada
	      uno a partir del anterior. A diferencia del método de \textit{bagging},
	      \textit{boosting} trabaja siempre sobre el mismo conjunto de datos y la
	      variabilidad está dada por los pesos que son asignados. El proceso es el
	      siguiente: para el primer modelo de clasificación, $M_{i}$, los pesos son
	      inicializados en un mismo valor para todas las tuplas. Una vez que se
	      entrena este modelo, los pesos son actualizados de tal manera que el
	      siguiente clasificador $M_{i} + 1$ trate de manera particular a las tuplas
	      mal clasificadas por $M_{i}$. De ese modo se busca llegar a una
	      clasificación correcta en las sucesivas iteraciones.  Finalmente, el
	      modelo de ensamble combina los votos de cada clasificador individual. Cabe
	      notar que el peso de cada voto también es ponderado de acuerdo al
	      rendimiento del clasificador base.


	\item[Stacking] La técnica de \textit{stacking} fue desarrollada por
	      \citeauthor{wolpert_stacked_1992}\cite{wolpert_stacked_1992} y consiste en
	      entrenar un nuevo clasificador de acuerdo a las predicciones realizadas
	      por otros modelos, tomando la salida de estos modelos como entrada, de tal
	      manera de lograr hallar una combinación que produzca una mejor predicción.
	      Este tipo de ensambles puede ser visto como un conjunto de capas. La
	      primera capa consta de un ensamble de clasificadores que aprenden a partir
	      de los datos de entrenamiento. Esta capa no necesariamente usa
	      clasificadores del mismo tipo, mismos hiperparámetros o particiones de la
	      colección iguales, quedando estos detalles a cargo de quien diseña esta
	      capa. La siguiente capa es el clasificador individual, o
	      meta-clasificador, que se alimenta de las salidas de los clasificadores de
	      la capa inferior y realiza el aprendizaje a partir de las clases
	      producidas por estas salidas y las clases reales.

\end{description}

Una de las tareas a tener en cuenta durante el entrenamiento de un ensamble es
la de combinar las salidas de cada modelo en una salida final. La estrategia más
común y simple es la de votación por mayoría, la cual normalmente es aplicada por los
métodos de \textit{bagging}. No obstante, existen múltiples métodos de combinar
los votos, e incluso no siempre un ensamble de tipo \textit{bagging} debe
aplicar esta estrategia. Por ejemplo, algunos clasificadores pueden decidir
producir una salida solo en el caso de que más de la mitad de ellos coincidan, o
incluso ser más restrictivos y obligar a que la coincidencia sea total. El
enfoque de \textit{boosting} por su parte, pondera al voto de acuerdo a los
pesos que calcula, dando predominio a determinadas instancias.  También se suele
dar un mayor peso a determinados clasificadores por sobre otros. Este tipo de
métodos se los denomina \comillas{mayoría de voto ponderada} y pueden llevar a
un rendimiento superior si es aplicada en el escenario adecuado.

\todo[inline]{Aplicaciones de ensambles en la literatura}


\subsection{Evaluación}
\label{evaluacion_intro}

Llevar a cabo evaluaciones de rendimiento sobre los modelos es un aspecto
importante del aprendizaje automático, ya que nos permite conocer en qué medida
un algoritmo es superior a otro para resolver una tarea. Particularmente, la
tarea de clasificación es un desafío que se presenta en un contexto cambiante y
evolutivo, donde nuevas herramientas surgen y se actualizan constantemente.
Incluso la composición y estructura de los modelos de clasificación varía según
la familia de algoritmos aplicados, y es esperable que los conceptos extraídos
de un modelo de tipo árbol tengan particularidades que lo diferencien de modelos
de redes neuronales o modelos probabilísticos. Y del mismo modo, es esperable
que alguno de estos modelos tenga un mejor rendimiento que otro en un
determinado escenario, o incluso que sea mejor que un modelo generado por el
mismo algoritmo pero con distintos hiperparámetros. La tarea de evaluación es
la que permite detectar estas particularidades y sacar provecho de los
algoritmos para obtener aún mejores modelos.

A su vez, es importante estudiar las métricas de evaluación existentes y llevar
adelante estrategias que nos permitan obtener medidas de evaluación confiables y
que no hayan sido sesgadas por los datos que se usaron durante el entrenamiento.
Y del mismo modo, entender los factores que provocaron un valor de métrica puede
ser el paso inicial para hallar mejoras al modelo que optimizan su capacidad de
predicción en el futuro.

Por consiguiente, a continuación se estudian algunas de las estrategias llevadas
a cabo durante la evaluación para evitar sesgos, así como también las métricas
que se calculan durante este proceso.

\subsubsection{Métricas}
\label{evaluacion_metricas}

Las métricas de evaluación se usan para conocer la habilidad predictiva de un
modelo de clasificación. Se van a describir aquí las métricas más conocidas del
campo y que proveen, a su vez, un primer vistazo de lo que serán las métricas
usadas para evaluar algoritmos para datos multi-etiquetados en las futuras
secciones del escrito.

Antes de comenzar es importante aclarar algunos términos que serán usados para
definir las métricas. Se entiende como \comillas{ejemplos positivos} a aquellas
instancias cuya etiqueta pertenece a la clase de interés en el problema de
estudio y \comillas{ejemplos negativos} como aquellas que no pertenecen a dicha
clase. Al mismo tiempo, se derivan cuatro conceptos que estarán presentes en las
fórmulas para calcular las métricas, los cuales son:

\begin{description}

	\item[\acrfull{vp}] Son los ejemplos positivos que fueron correctamente
	      clasificados como positivos.

	\item[\acrfull{vn}] Son los ejemplos negativos que fueron correctamente
	      clasificados como negativos.

	\item[\acrfull{fp}] Son los ejemplos negativos que fueron incorrectamente
	      clasificados como positivos.

	\item[\acrfull{fn}] Son los ejemplos positivos que fueron incorrectamente
	      clasificados como negativos.

\end{description}

Una vez obtenidos estos conceptos se pueden calcular una serie de métricas a
partir de ellos. Estas métricas son:

\paragraph{Exactitud}
\label{evaluacion_metricas_exactitud}

La exactitud o \textit{accuracy} es la proporción de ejemplos correctamente
clasificados sobre el número total de instancias y a mayor el valor de exactitud
mejor es el rendimiento del clasificador. Se define  como:

\begin{equation}
	exactitud = \frac{\acrshort{vp} + \acrshort{vn}}{\acrshort{vp} +
		\acrshort{vn} + \acrshort{fp} + \acrshort{fn}}
\end{equation}

\paragraph{Tasa de Error}

A la inversa de la exactitud, la tasa de error es la proporción de ejemplos
incorrectamente clasificados sobre el número total de instancias y a menor el
valor de la tasa de error mejor es el rendimiento del clasificador. Se define
como:

\begin{equation}
	tasaError = \frac{\acrshort{fp} + \acrshort{fn}}{\acrshort{vp} +
		\acrshort{vn} + \acrshort{fp} + \acrshort{fn}}
\end{equation}

\paragraph{Precisión}

La precisión es la proporción de ejemplos que fueron clasificados como positivos
y que efectivamente lo son. Mayor es el valor de precisión mejor es el
rendimiento del clasificador. Se dice que es una medida de exactitud y se define
como:

\begin{equation}
	precision = \frac{\acrshort{vp}}{\acrshort{vp} + \acrshort{fp}}
\end{equation}

\paragraph{Exhaustividad}

La exhaustividad o \textit{recall} es la proporción de ejemplos positivos que
fueron clasificados como positivos. Mayor es el valor de exhaustividad mejor es
el rendimiento del clasificador. Se dice que es una medida de completitud y se
define como:

\begin{equation}
	exhaustividad = \frac{\acrshort{vp}}{\acrshort{vp} + \acrshort{fn}}
\end{equation}

\paragraph{Medida-F1}

La medida-F1 o \textit{f1-score} es una medida que integra las métricas de
precisión y exhaustividad tomando la media harmónica entre ambas. Mayor el valor
de esta medida, mejor es el rendimiento del clasificador. Se define como:

\begin{equation}
	medidaF1 = \frac{2 \times precision \times exhaustividad}
	{precision + exhaustividad}
\end{equation}

Por otro lado, existen métricas para evaluar la eficiencia del clasificador en
términos de velocidad y consumo de memoria. Estas métricas cobran especial
importancia en ambientes de flujos continuos de datos, donde el volumen de datos
es grande, la velocidad en la que arriban es alta y los recursos escasean. A
continuación se añade una breve descripción de ambas.

\paragraph{Velocidad}

La velocidad se refiere al costo computacional de generar el modelo y realizar
predicciones, en general se deja fuera del cálculo los tiempos invertidos en
cargar la colección en memoria, realizar tareas de normalización y
preprocesamiento sobre los datos y otras etapas de la clasificación. A menor el
tiempo de ejecución, mejor es el rendimiento del algoritmo.

\paragraph{Consumo de Memoria}

El consumo de memoria es un indicador de los requerimientos de memoria
aproximados para almacenar el modelo, así como también un indicador de resguardo
para estimar el posible consumo de memoria durante la ejecución y asegurarse que
el algoritmo se mantiene en actividad. A menor el consumo de memoria, mayor es
la eficiencia del modelo.

\subsubsection{Estrategias}
\label{evaluacion_estrategias}

Una vez que se define la métrica o el conjunto de métricas apropiadas para
medir el rendimiento de los clasificadores, el siguiente desafío es seguir un
procedimiento de pruebas capaz de lograr resultados de evaluación que puedan ser
generalizables a conjunto de datos aún no observados. Las técnicas de
\comillas{\textit{Holdout}} y \comillas{Validación Cruzada} son dos de las
técnicas más populares para evaluar la habilidad predictiva de los
clasificadores y se describen a continuación.


\paragraph{\textit{Holdout}}

En este método un conjunto de instancias es separado de la colección y se
reserva para evaluar el rendimiento del clasificador.  Este subconjunto es
distinto del conjunto de datos de entrenamiento usado para generar el modelo y
es llamado \comillas{conjunto de pruebas o \textit{testing}}. Una vez entrenado,
el clasificador recibe las instancias del conjunto de pruebas, pero sin incluir
las etiquetas. La salida del clasificador son las etiquetas de cada instancia.
Finalmente, las etiquetas producidas durante la predicción y las etiquetas
reales de cada instancia se combinan y se calculan las métricas de evaluación.
Esta técnica se basa en la idea de que, separando los datos que se usan durante
el entrenamiento de aquellos usados durante la predicción, se logra una
independencia en los datos que derivará en un mayor grado de generalización en
el modelo.

Por contrapartida, el enfoque de \textit{Holdout} tiene la limitación de que
para lograr generalizar requiere de un número de instancias considerable.
\todo{citar fuente} Esta idea proviene del hecho de que muy pocos datos en el
conjunto de entrenamiento puede derivar en predicciones pobres, pero, por el
contrario, muy pocos datos en el conjunto de pruebas tampoco es recomendable, ya
que podría resultar en medidas de rendimiento poco fiables. A esto se suma una
de las dificultades más comunes en el área de aprendizaje automático: la falta
de disponibilidad de colecciones grandes de datos del mundo real.

Por estas razones, ha tomado peso el uso de técnicas de muestreos de datos para
reutilizar instancias de entrenamiento y de prueba. Una de ellas es la de
validación cruzada.

\paragraph{Validación Cruzada de K iteraciones}

La técnica de validación cruzada, más conocida por el inglés \textit{k-fold
	cross-validation}, consiste en particionar el conjunto de datos en $k$
subconjuntos de manera aleatoria y en la forma $\{d_{1}, d_{2},  \dots, d_{k}
	\}$, siendo cada uno de los subconjuntos mutuamente excluyentes entre sí y de
igual o similar tamaño. El proceso itera sobre cada uno de los subconjuntos para
generar $k$ modelos. En la primera iteración $i$, se separa el subconjunto
$d_{i}$ y se entrena el modelo con los restantes subconjuntos para luego medir
el rendimiento con él. En la segunda iteración, se repite este procedimiento
pero usando como pruebas al subconjunto $d_{i+1}$, y así en cada iteración. Así
pues, cada subconjunto es usado una vez para probar el modelo. Finalmente, se
toman los $k$ modelos generados y se promedian las métricas.

De esta técnica derivan otras similares como \comillas{\textit{leave-one-out}}
en donde cada subconjunto es conformado por $n-1$ instancias, siendo $n$ el
tamaño de la colección, dejando un elemento fuera del subconjunto y que será
usado para validar el modelo. El proceso se repite $n$ veces y se combinan los
modelos tal como en el método de validación cruzada.

Otra técnica similar es la de \comillas{Validación Cruzada Estratificada}, la
cual consiste en generar subconjuntos de entrenamiento y pruebas que respeten la
representación de clases existentes en la colección inicial. Por ejemplo, si una
de las clases del problema aparece en el $25\%$ de las instancias, este método
asegura que al generar las particiones esa clase continuará siendo representada
por el $25\%$ de las tuplas. Esta técnica puede ser útil para problemas donde
existe una diversidad que es necesario reflejar en las particiones para obtener
medidas precisas.

En general, los investigadores recomiendan usar validación cruzada con $k=10$ ya
que se suelen conseguir estimaciones menos sesgadas sin incurrir en costos
computacionales demasiado altos.

\section{Clasificación Multi-etiquetas}

A diferencia del aprendizaje automático tradicional, que usa datos de etiqueta
única para representar objetos del mundo real, cada instancia en el aprendizaje
multi-etiquetas representa un único objeto, pero puede contener más de una
etiqueta. Por consiguiente, la tarea de clasificación consiste en hallar una
función que logre asignar a cada objeto, nuevo y desconocido, el conjunto de
etiquetas que lo caracteriza.

En este apartado se da una definición formal, se detallan las características de
un conjunto de datos multi-etiquetados y se  describen algunos métodos
tradicionales de clasificación multi-etiquetas junto con sus ventajas,
desventajas, aplicaciones y motivaciones.

\subsection{Definición}
\label{mll_def_formal}

Asumiendo que $X=\mathbb{R}^{d}$ denota el espacio de instancias $d$
dimensional, y que $Y = \{y_{1}, y_{2}, \dots, y_{q}\}$ denota el espacio de
etiquetas con $q$ etiquetas posibles, la tarea de clasificación multi-etiquetas
consiste en entrenar un conjunto $D = \{(x_{i}, Y_{i}) \mid 1 \leq i \leq m\}$
para hallar una función $h$ tal que $h: X \rightarrow 2^y$. A su vez, $X_{i}$ es
un vector de atributos $d$ dimensional definido como $(x_{i1}, x_{i2}, \dots,
	x_{id})$. $Y_{i}$, por su parte, es el conjunto de etiquetas asociadas a la
instancia $X_{i}$. Luego, para cada instancia desconocida $x \in X$ el
clasificador $h$ predice $h(x) \subseteq Y$ que representa el conjunto de
etiquetas hallado para $x$.

A su vez, se definen un conjunto de métricas que describen el grado de
multi-etiquetado que tiene un conjunto de datos dado, o en otras palabras, hasta
qué punto cada ejemplo posee más de una etiqueta. Algunas de ellas son:

\begin{description}
	\item{Cardinalidad de etiquetas}: Es el promedio de etiquetas por instancia
	      del conjunto de datos. Se define como:
	      \begin{equation}
		      \label{eq:mll_card}
		      CardE(D) = \frac{1}{m} \sum_{i=1}^{m} \left\|Y_{i}\right\|
	      \end{equation}
	      Por lo tanto, a mayor el valor de cardinalidad, mayor es el número de
	      etiquetas de una instancia. Por ejemplo, si $CardE = 1$, entonces la
	      mayoría de ejemplos tiene una única etiqueta y, por consiguiente, se puede
	      decir que la colección tiene un grado bajo de multi-etiquetado.
	\item{Densidad de etiquetas}: Es la cardinalidad de etiquetas normalizada al
	      número total de etiquetas de $D$ y se define como:
	      \begin{equation}
		      DenE(D) = \frac{CardE(D)}{\left\|Y\right\|}
	      \end{equation}
	      Así pues, un valor alto de densidad significaría que cada instancia puede
	      ser una buena representación de las etiquetas del conjunto. De la misma
	      manera, un valor bajo suele implicar dispersión, esto es, que la mayoría
	      de las instancias tienen un subconjunto acotado de las etiquetas.
	\item{Diversidad de etiquetas}: Es el número de conjuntos de etiquetas
	      unívocos que aparecen en instancias de $D$. Se define como:
	      \begin{equation}
		      DivE(D) = \left\|\{Y \mid \exists x: (x, Y) \in D\}\right\|
	      \end{equation}
	      Aquí la interpretación es que, a mayor el valor de diversidad, menor es la
	      constancia con la que las etiquetas aparecen en las instancias. De manera
	      similar a la cardinalidad, el valor de diversidad también puede
	      normalizarse por el número de instancias del conjunto de datos:
	      \begin{equation}
		      DivEProm(D) = \frac{DivE(D)}{\left\|D\right\|}
	      \end{equation}
\end{description}

\subsection{Algoritmos}

Como se había anticipado en la sección~\ref{intro_mll}, la tarea de aprendizaje
sobre datos multi-etiquetados puede ser encarada siguiendo dos grandes enfoques,
llamados \comillas{Transformación del Problema} y \comillas{Adaptación del
	Algoritmo}. A continuación se describen ambos enfoques y algunos de sus
algoritmos más representativos.

\subsubsection{Transformación del Problema}

Esta categoría engloba al conjunto de algoritmos que abordan el problema de
clasificación multi-etiquetas transformándolo en múltiples problemas de
clasificación de única etiqueta, lo cual permite aplicar algoritmos de
clasificación convencionales. Tres de estos métodos son particularmente
relevantes para este trabajo: \comillas{\acrfull{br}}, \comillas{\acrfull{cc}} y
\comillas{\acrfull{lp}}.

\paragraph{\acrfull{br}}

El algoritmo de Relevancia Binaria, conocido como \textit{\acrlong{br}} en la
literatura, es un enfoque que consiste en descomponer la tarea de clasificación
\acrshort{mll} en $\left\|q\right\|$ clasificadores binarios, independientes y
de etiqueta única.  A partir de esta transformación se puede seleccionar
cualquier algoritmo de clasificación como clasificador base del problema (ver
los algoritmos presentados en la sección~\ref{clasificacion_algoritmos}).  Cada
clasificador binario $g_{j}$ es entrenado con todas las instancias de la
colección, pero incluyendo solo la etiqueta $j$, la cual se activa o desactiva de
acuerdo a si es relevante a la instancia. Luego la predicción de una instancia
desconocida se realiza combinando las salidas de cada clasificador individual,
esto es:

\begin{equation}
	Y = {y_{j} \mid g_{j}(x) > 0, 1 \leq j \leq q}
\end{equation}

Llegado el caso en que ninguno de los clasificadores retornen etiquetas activas,
el conjunto $Y$ será vacío.

Este enfoque se dice que es de primer orden (ver sección~\ref{estrategias_mll})
y no tiene en cuenta la correlación o interdependencias entre etiquetas. Este es
uno de los principales inconvenientes de este algoritmo porque, en este tipo de
problemas de \acrshort{mll}, es usual hallar que determinadas etiquetas se
activan en conjunto con mayor probabilidad. Pese a ello, \acrshort{br} es un
enfoque muy utilizado \cite{zhang_review_2014} ya que es simple de implementar,
intuitivo y computacionalmente poco costoso en comparación con algoritmos que sí
tienen en cuenta la relación entre etiquetas.

\paragraph{\acrfull{cc}}

Las cadenas de clasificadores o \textit{\acrlong{cc}}
\cite{read_classifier_2011} es una técnica que convierte el problema de
\acrshort{mll} en una \comillas{cadena} de problemas de clasificación binaria,
tal que el siguiente clasificador de la cadena posee las predicciones de los
anteriores. En principio, la división del conjunto de datos es similar a la que
se hace en el enfoque anterior, designando un clasificador por cada etiqueta.
Durante el entrenamiento, el clasificador inicial, seleccionado aleatoriamente,
usa de entrada los atributos originales, tal como el clasificador \acrshort{br}.
Luego la salida de este clasificador es añadida al espacio de atributos como un
atributo más de cada instancia, para que posteriormente, estos atributos sean la
entrada del siguiente clasificador, el cual también es seleccionado al azar.
Este proceso es repetido hasta completar todos los clasificadores.  Como se
puede observar, lo que se produce es un \comillas{encadenamiento} de
clasificadores, el cual no es accidental y tiene como fin conservar la
dependencia entre etiquetas: en el entrenamiento se van acumulando las salidas
de los clasificadores anteriores de tal manera que el siguiente clasificador
absorbe la correlación entre las etiquetas de los anteriores clasificadores.

Cabe notar que en esta técnica cobra especial importancia el ordenamiento de los
clasificadores, ya que este orden tiene un impacto directo sobre el resultado de
la predicción. En otras palabras, si el ordenamiento de clasificadores se
modifica, el modelo final otorgará resultados diferentes. Para salvar esta
dificultad se han propuesto modelos como \acrfull{ecc}. El mismo genera un
conjunto de modelos de \acrshort{cc} con distintos ordenamientos y entrenados
con diversos subconjuntos de datos, generados con reemplazo o no. Durante la
predicción, cada cadena produce un conjunto de etiquetas, que son los votos, y
la salida final será computada por un algoritmo que combine cada salida
individual.

\paragraph{\acrfull{lp}}

El conjunto de potencias de etiquetas o \textit{\acrlong{lp}}
\cite{tsoumakas_random_2011} es una técnica que se encarga de transformar el
problema de \acrshort{mll} en uno de clasificación multi-clase y así poder
abordarlo con algoritmos de este tipo. La clasificación multi-clase es un
enfoque usado para tratar con ejemplos en donde la etiqueta es única, pero cuenta
con más de dos clases. Un ejemplo de este tipo de problemas es el de análisis de
sentimiento de texto, en donde las clases pueden ser \comillas{positivo},
\comillas{negativo} y \comillas{neutral}.

En \acrshort{lp} cada etiqueta indica el subconjunto de etiquetas de la
instancia. Esto es beneficioso en cuanto a que se logra preservar la dependencia
entre etiquetas. Sin embargo, el modelo tiene algunas dificultades.  En primer
lugar, el espacio de clases posibles es exponencial y su cantidad de clases
puede llegar a ser de $2^{\left\|q\right\|}$ como máximo. A su vez, pueden
llegar a arribar ejemplos con una combinación de etiquetas que el modelo no
recibió durante el entrenamiento, por lo cual no logra generalizar lo suficiente
y se lo considera un modelo incompleto. A fin de sobrepasar estas complicaciones
se desarrolló la técnica de \comillas{Conjuntos Podados} o
\comillas{\acrfull{ps}}. La misma consiste en preservar para la clasificación
aquellos subconjuntos de etiquetas que son más frecuentes en la colección, y
eliminar los demás. Con esto se logra disminuir considerablemente el espacio de
clases y disminuye la complejidad computacional, tanto durante el entrenamiento
como durante la predicción.

\subsubsection{Adaptación del Algoritmo}

Además del enfoque de transformación del problema, algunos autores abordan la
clasificación multi-etiquetas a partir de la adaptación de algoritmos clásicos y
bien conocidos a este tipo de escenarios. La categoría engloba al conjunto de
algoritmos que acometen el problema de \acrshort{mll} mediante la modificación
de algoritmos de etiqueta única para que sean capaces de manejar la nueva
naturaleza de los datos en estas tareas. Las modificaciones que se introducen
pueden variar en complejidad según el algoritmo tratado y las características de
la colección.  Se han adaptado una diversa cantidad de algoritmos incluyendo
aquellos basados en redes neuronales, árboles, métodos probabilísticos, entre
otros \cite{herrera_multilabel_2016}.  Por mencionar algunos ejemplos,
\citeauthor{gargiulo_deep_2018} usan redes neuronales profundas para clasificar
documentos de texto libre y comparan su funcionamiento variando el número de
etiquetas y aprovechando su estructura jerárquica \cite{gargiulo_deep_2018}.
Asimismo, \citeauthor{tanaka_multi-label_2015} agregan al algoritmo de
\acrshort{br} la capacidad de capturar relaciones entre etiquetas a través del
uso de árboles de decisión y lo aplican en el área de la genómica
\cite{tanaka_multi-label_2015}.

En lo que confiere a ambientes de flujos continuos de datos una de las técnicas
más populares en la literatura es la de Árbol de Hoeffding o
\textit{\acrfull{ht}} \cite{domingos_mining_2002}, que a diferencia de los
algoritmos convencionales de árboles de decisión, aborda los datos de manera
incremental. Así pues, en lugar de realizar decisiones de corte de acuerdo a los
datos previamente almacenados, el algoritmo de \acrshort{ht} espera a tener una
cantidad suficiente de instancias para realizar el corte, con un cierto grado de
confianza. Esto significa que ya no es necesario guardar todos los datos de la
colección y que mantener una serie de estadísticas es suficiente para realizar
la clasificación. Otra de las propiedades a destacar de este método es que en
teoría se puede entrenar un árbol cuyo rendimiento se aproxime al generado en un
ambiente de \textit{batch}, con la suficiente cantidad de datos
\cite{bifet_machine_2018}.

A partir de esto, y buscando sacar provecho de las ventajas mencionadas,
\citeauthor{read_scalable_2012} decidieron adaptar este algoritmo a problemas de
multi-etiquetas y desarrollaron el algoritmo llamado Árbol de Hoeffding
Multi-etiquetado o \textit{\acrfull{mlht}} \cite{read_scalable_2012}. Esto lo
consiguen a partir de rediseñar la fórmula de ganancia de información (ver
fórmula~\ref{eq:gan_c45}) de tal manera de reflejar en el cálculo de la entropía
el impacto de todas las clases a las cuales el ejemplo no pertenece. Desde que
fue introducido en el año \citeyear{read_scalable_2012}, \acrshort{mlht} ha sido
usado como modelo de comparación en reiteradas oportunidades
\cite{sousa_multi-label_2018} y es uno de los métodos más populares para atacar
problemas de \acrshort{mll} en ambientes de flujo continuo de datos.

Otro algoritmo muy popular para abordar este tipo de problemas y que también se
basa en árboles de decisión incrementales es el llamado \acrshort{isoup} (siglas
del inglés \textit{\acrlong{isoup}}) \cite{osojnik_multi-label_2017}. El mismo
fue desarrollado inicialmente para hacer regresión de múltiples objetivos y
luego fue adaptado a tareas de \acrshort{mll}. Una de las novedades que
introduce es el uso de un perceptrón adaptativo en las hojas del árbol, lo cual
le da la versatilidad de poder trabajar tanto con problemas de clasificación
como de regresión.

\todo[inline]{Explicación de enfoques de ensambles? (ebr, ecc, elp)}

\subsection{Evaluación}
\label{mll_evaluacion}

Como se describió en la sección~\ref{evaluacion_metricas}, en escenarios de
clasificación tradicionales el poder de generalización de un modelo es evaluado
a partir de métricas tales como la exactitud o la exhaustividad las cuales
trabajan con una única etiqueta. Una predicción multi-etiquetada, por el
contrario, carga con la complejidad de que cada ejemplo puede ser asociado a más
de una o dos etiquetas. Esto implica que ya no se puede trabajar con la
presunción de que una clasificación solo puede ser correcta o incorrecta. De lo
contrario, se incursionaría en una evaluación excesivamente rigurosa. Por este
motivo, en el campo de \acrshort{mll} se han diseñado una serie de métricas
nuevas y específicas para abordar este tipo de problemas, y se clasifican en dos
grupos: existen las métricas \comillas{basadas en ejemplos}, que son computadas
por cada instancia y luego promediadas; y las métricas \comillas{basadas en
	etiquetas}, las cuales son calculadas para cada etiqueta por separado y
devuelven el promedio micro o el promedio macro.

A continuación se describe en detalle ambas categorías, incluyendo métricas
presentadas por cada enfoque, junto con su definición formal y una
interpretación de la evaluación que proveen.

\subsubsection{Métricas basadas en Ejemplos}

En esta categoría se hallan las métricas que evalúan la calidad de la
clasificación ejemplo por ejemplo y determinan qué tan buena es la clasificación
sobre los distintos ejemplos. Tienen en común que comparan las etiquetas de la
predicción contra las etiquetas reales para cada instancia y luego combinan la
salida aplicando la media sobre el total de instancias. Entre ellas se
encuentran las siguientes:

\paragraph{\textit{Hamming Loss}}

Esta es una de las métricas más populares en la literatura y se encarga de
evaluar la proporción de pares instancia-etiqueta incorrectamente clasificados,
es decir, toma en cuenta para el conteo tanto si se marcó como irrelevante una
etiqueta relevante o si, por el contrario, se activó una etiqueta que no era
relevante. Se define como:

\begin{equation}
	HLoss = \frac{1}{m} \sum_{i=1}^{m} \left\|h(x_{i}) \triangle Y_{i}\right\|
\end{equation}

Aquí el símbolo \comillas{$\triangle$} es la diferencia simétrica entre
conjuntos. Tener en cuenta también que cuanto menor es el valor de
\textit{hamming loss} mejor es el rendimiento del clasificador.

De manera complementaria, algunos autores de la literatura han decidido usar la
métrica \comillas{\textit{hamming score}} que es un derivado del \textit{hamming
	loss} y se calcula como:

\begin{equation}
	HScore = 1 - HLoss
\end{equation}

\paragraph{Exactitud del Subconjunto} Se la conoce también como
\textit{exact-match} en la literatura y es la métrica equivalente a la exactitud
en clasificaciones de etiqueta única (ver \comillas{Exactitud} en
sección~\ref{evaluacion_metricas_exactitud}). El cómputo toma en cuenta la
proporción de instancias que fueron clasificadas de manera exacta, es decir, que
todas sus etiquetas fueron correctamente clasificadas o, en otras palabras, que
el subconjunto de etiquetas de la predicción es idéntico al subconjunto de
etiquetas reales. En términos formales, se calcula como:

\begin{equation}
	exactitudSubconjunto = \frac{1}{m} \sum_{i=1}^{m} \left\|h(x_{i}) =
	Y_{i}\right\|
\end{equation}

Es considerada una métrica excesivamente rigurosa, especialmente para
colecciones donde el número de etiquetas es alto.

\paragraph{Exactitud basada en ejemplos}

La exactitud basada en ejemplos también es conocida como \comillas{Índice de
	Jaccard} y consiste en tomar la proporción de etiquetas activas y correctamente
clasificadas con respecto al total de etiquetas activas.

\begin{equation}
	exactitudEj = \frac{1}{m} \sum_{i=1}^{m}
	\frac{\left\|Y_{i} \cap h(x_{i})\right\|}
	{\left\|Y_{i} \cup h_(x_{i})\right\|}
\end{equation}


\paragraph{Precisión, Exhaustividad y Medida-F1 (basadas en ejemplos)}

Las métricas de precisión, exhaustividad y medida-F1 presentadas en la
sección~\ref{evaluacion_metricas} también tienen sus equivalentes para datos
multi-etiquetados y se definen de la siguiente manera:

\begin{equation}
	precisionEj = \frac{1}{m} \sum_{i=1}^{m}
	\frac{\left\|Y_{i} \cap h(x_{i})\right\|}
	{\left\|h_(x_{i})\right\|}
\end{equation}

\begin{equation}
	exhaustividadEj = \frac{1}{m} \sum_{i=1}^{m}
	\frac{\left\|Y_{i} \cap h(x_{i})\right\|}
	{\left\|Y_{i}\right\|}
\end{equation}

\begin{equation}
	medidaF1Ej = \frac{2 \times precisionEj \times exhaustividadEj}
	{precisionEj + exhaustividadEj }
\end{equation}


\subsubsection{Métricas basadas en Etiquetas}

Hasta aquí, todas las métricas mencionadas son calculadas individualmente para
cada instancia y luego promediadas por el total de instancias. No obstante,
también existen otros tipos de métricas, las basadas en etiquetas, que se
encargan de evaluar la calidad de la clasificación etiqueta por etiqueta y
determinan qué tan buena es la clasificación sobre las distintas etiquetas.  En
síntesis, comparan las etiquetas de la predicción contra las etiquetas reales,
cada una por separado y luego combinan el rendimiento entre todas las etiquetas.
Dicha combinación se realiza aplicando el promedio micro o el promedio macro.
Más adelante se explicará en qué consiste cada una de ellas.

De manera similar a lo que sucedía con las métricas de la
sección~\ref{evaluacion_metricas}, donde para definirlas fue necesario
introducir el concepto de verdaderos positivos, verdaderos negativos, falsos
negativos y falsos positivos, las métricas basadas en etiquetas también se
fundamentan en esos conceptos, pero su fórmula varía para adaptarse al escenario
de \acrshort{mll}.

\begin{equation}
	\acrshort{vp}_{j} = \left\|\{x_{i}\mid y_{j} \in Y_{i} \land y_{j} \in
	h(x_{i}), 1 \leq i \leq m\}\right\|
\end{equation}

\begin{equation}
	\acrshort{fp}_{j} = \left\|\{x_{i}\mid y_{j} \notin Y_{i} \land y_{j} \in
	h(x_{i}), 1 \leq i \leq m\}\right\|
\end{equation}

\begin{equation}
	\acrshort{vn}_{j} = \left\|\{x_{i}\mid y_{j} \notin Y_{i} \land y_{j} \notin
	h(x_{i}), 1 \leq i \leq m\}\right\|
\end{equation}

\begin{equation}
	\acrshort{fn}_{j} = \left\|\{x_{i}\mid y_{j} \in Y_{i} \land y_{j} \notin
	h(x_{i}), 1 \leq i \leq m\}\right\|
\end{equation}

Tal como sucede con el aprendizaje tradicional, bajo esta definición las
evaluaciones basadas en etiquetas logran satisfacer la condición:

\begin{equation}
	\acrshort{vp} + \acrshort{fp} + \acrshort{vn} + \acrshort{fn} = m
\end{equation}

A partir de estos cuatro conceptos se pueden derivar cualquiera de las métricas
definidas en la sección~\ref{evaluacion_metricas}. O dicho en términos formales,
sea $B(\acrshort{vp}_{j}, \acrshort{fp}_{j}, \acrshort{vn}_{j},
	\acrshort{fn}_{j})$ una función cuyo dominio es $B \in \{ exactitud, precision,
	exhaustividad, medidaF1 \}$, las métricas pueden ser obtenidas siguiendo dos
estrategias:

\paragraph{Promedio Macro}

\begin{equation}
	B_{macro}(h) = \frac{1}{q} \sum_{j=1}^{q}
	B(\acrshort{vp}_{j}, \acrshort{fp}_{j}, \acrshort{vn}_{j}, \acrshort{fn}_{j})
\end{equation}

\paragraph{Promedio Micro}

\begin{equation}
	B_{micro}(h) = B( \sum_{j=1}^{q} \acrshort{vp}_{j}, \sum_{j=1}^{q}
	\acrshort{fp}_{j},\sum_{j=1}^{q}  \acrshort{vn}_{j},\sum_{j=1}^{q}
	\acrshort{fn}_{j})
\end{equation}

El enfoque de promedio macro tiene una semejanza al enfoque basado en ejemplos
en cuanto a que ambas estrategias promedian por el mismo valor a todas sus
etiquetas/ejemplos, siendo en este caso el número total de etiquetas. Esto se
traduce en que esta categoría de métricas le asigna a cada etiqueta el mismo
peso, sin importar la frecuencia de la misma.

Por el contrario, el enfoque de promedio micro sí provoca una ponderación en las
etiquetas y la contribución de cada etiqueta al valor final no será el mismo.
Esto sucede porque la métrica se computa solo una vez, habiendo ya calculado la
cantidad de etiquetas para cada uno de los cuatro conceptos, lo que hace que
aquellas etiquetas que se activan en escasas ocasiones vayan a tener una menor
injerencia y, por tanto, serán sobrepasadas por aquellas etiquetas más frecuentes.

\section{Clasificación de Flujos Continuos de Datos}

En esta sección se profundiza sobre algunos conceptos referidos a flujos
continuos de datos y su tratamiento, de forma tal de llevar a cabo tareas de
clasificación de manera exitosa.  Como se describió en la
sección~\ref{intro_streams}, un flujo continuo cuenta con características
específicas que obligan a los algoritmos de aprendizaje a adaptarse a nuevos
requerimientos.  A continuación se define formalmente el concepto de flujo, se
extiende el análisis sobre la tarea de evaluación para dar cuenta de su
aplicación en este escenario, se describe el concepto de dato sintético y se
presentan algunos algoritmos y técnicas para generar datos sintéticos útiles
para clasificaciones de etiqueta única y de multi-etiquetas.

\subsection{Definición}

Un flujo continuo de datos o \textit{Data Stream} es un conjunto de datos
ordenados, que arriban en el tiempo y son potencialmente infinitos. Un
\textit{stream} se define como:

\begin{equation}
	S = \{s_{0}, s_{1}, \dots, s_{t}, \dots, s_{N}\}
\end{equation}

Donde $s_{t}$ es la instancia presente en el tiempo $t$ y $s_{N}$ es el último
dato avistado en el \textit{stream}, pero que no necesariamente representa el
final del flujo. Cada instancia $d_{t}$ posee un conjunto de atributos y
etiquetas y se puede definir tal como se hizo en la sección~\ref{clasificacion}
de tratarse de datos uni-etiquetados o como en la sección~\ref{mll_def_formal}
de tratarse de datos multi-etiquetados.

El objetivo de la tarea de clasificación en este escenario es el mismo que para
escenarios de \textit{batch}, esto es, hallar una función capaz de enlazar
instancias nuevas con sus etiquetas correspondientes, con la salvedad de que se
debe lograr bajo restricciones de tiempo y espacio de almacenamiento, además de
otras vicisitudes presentadas por las características mismas de un flujo
continuo de datos (ver sección~\ref{stream_caracteristicas}). Del mismo, deben
ser capaces de lidiar con cambios en la distribución de los datos, deben estar
listos para realizar predicciones en cualquier momento y otros requisitos ya
mencionados en la sección~\ref{stream_requisitos}. Estas características
implican que obtener soluciones exactas es poco probable y es necesario aplicar
técnicas y metodologías especiales de evaluación sobre los modelos, para reducir
el error posible.

\todo[inline]{sección para explicar concept drift?}

\subsection{Evaluación}
\label{stream_evaluacion}

Como se describió en la sección~\ref{evaluacion_intro}, la tarea de evaluación
para aprendizaje en ambientes de \textit{batch} consiste en dividir al conjunto
de datos en uno o más subconjuntos de entrenamiento y pruebas.  El ambiente de
\textit{stream}, por su parte, presenta un desafío extra y es que no permite
llevar a cabo esta división debido a que no se cuenta con los datos previamente
almacenados. Además, se debe tener en consideración la naturaleza incremental y
evolutiva de los datos: a medida que pasa el tiempo pueden surgir nuevas
etiquetas, ciertos atributos pueden dejar de tener peso en la predicción o
incluso algunas reglas de decisión en el modelo pueden llegar a perder
relevancia. Por lo tanto, se han presentado dos enfoques nuevos, similares a las
estrategias definidas en la sección~\ref{evaluacion_estrategias} pero
acondicionados a los escenarios de flujos continuos de datos.

\begin{description}

	\item[Evaluación \textit{holdout}] La evaluación por retención o
	      \textit{holdout} es un método derivado de la técnica de validación cruzada
	      pero adaptado a ambientes de flujos continuos. Consiste en usar una parte
	      del \textit{stream} como conjunto de entrenamiento y, periódicamente,
	      extraer una serie de conjuntos de prueba, llamados conjuntos de
	      \textit{holdout}, que son usados para computar las métricas de evaluación
	      y que, por tanto, no deben haber sido observados por el modelo
	      previamente.  \textit{Holdout} es un método que requiere contar con flujos
	      continuos lo suficientemente grandes para que la evaluación sea precisa,
	      lo cual no siempre es posible y es uno de los impedimentos que hacen que
	      esta estrategia tenga menor trascendencia que otras.

	\item[Evaluación \textit{Prequential}] La técnica de evaluación
	      \textit{Prequential} o \textit{test-then-train} consiste en realizar la
	      evaluación de cada instancia primero y luego usarla para actualizar el
	      modelo. En consecuencia, ya no hay una división en subconjuntos de datos
	      independientes, sino que todas las instancias son usadas para evaluar y
	      luego clasificar, en un mismo instante de tiempo. A diferencia del enfoque
	      de \textit{holdout}, no es necesario que el conjunto de datos sea grande y
	      la evaluación \textit{prequential} permite alimentar al modelo con todos
	      los datos de la colección, lo que se traduce en un aprovechamiento máximo
	      del \textit{stream}. Por estas razones, el enfoque \textit{prequential}
	      tiende a ser el más usado en este campo de estudio.

\end{description}

En cuanto a las métricas de rendimiento utilizadas, en el ámbito de
clasificaciones de flujos continuos se usan las mismas métricas de
clasificaciones por \textit{batch} con la salvedad que el cómputo se hace de
manera incremental. En otras palabras, a diferencia de los métodos clásicos de
validación cruzada donde la evaluación se hace en una única pasada, una vez
generado el modelo final, aquí se debe calcular la métrica en cada nueva
instancia y los resultados se van acumulando.

En definitiva, las métricas son las mismas que fueron presentadas en las
secciones~\ref{evaluacion_metricas} para datos de etiqueta única
y~\ref{mll_evaluacion} para datos multi-etiquetados.

\subsection{Datos sintéticos}
\label{stream_syn}

Generar datos sintéticos es una práctica frecuente en la literatura para simular
ambientes de flujos continuos de datos, el principal motivo es la falta de
colecciones de \textit{streams} del mundo real que sean lo suficientemente
grandes y que al mismo tiempo cumplan con todos los requisitos necesarios para
evaluar algoritmos en este escenario \cite{kirkby_improving_2007}. Pese a esta
restricción, se han hallado ventajas comparativas en la aplicación de flujos
sintéticos en el análisis y evaluación de algoritmos, entre ellas se encuentran
las siguientes:

\begin{itemize}

	\item Tiene un costo de almacenamiento relativamente menor.

	\item Cumplen con el requisito de ser teóricamente infinitos.

	\item Su generación es automatizable lo cual facilita su reproducibilidad
	      entre experimentos.

	\item Es posible introducir cambios de concepto artificiales para realizar un
	      análisis incisivo de los algoritmos de clasificación bajo escenarios
	      dinámicos y cambiantes.

	\item Ayudan en el ámbito académico y científico a realizar estudios y
	      experimentos más abarcativos.

\end{itemize}

En la actualidad existen varios generadores de flujos sintéticos que logran
cumplir con los requisitos necesarios, aquí se describen dos de ellos:
\comillas{\acrfull{rtg}} y \comillas{\acrfull{rbf}}.

\begin{description}

	\item[\textit{\acrfull{rtg}}] El generador se basa en la técnica presentada
	      por \citeauthor{domingos_mining_2002} \cite{domingos_mining_2002} y
	      consiste en producir un \textit{stream} partiendo de un árbol construido
	      aleatoriamente. A fines de generar el árbol, el método selecciona un
	      atributo al azar para realizar el corte y posteriormente le asigna una
	      clase aleatoria a cada hoja. A partir de este árbol se van a generar las
	      instancias sintéticas.  Primero se asignan valores aleatorios a los
	      atributos, siguiendo una distribución uniforme, y luego con esos valores se
	      atraviesa el árbol para hallar las clases de la etiqueta. Teniendo en
	      cuenta que las instancias son generadas y clasificadas según un modelo con
	      estructura de árbol, en teoría este método favorece a algoritmos del tipo
	      de árboles de decisión.

	\item[\textit{\acrfull{rbf}}] El generador produce un \textit{stream} de
	      función de base radial aleatoria. El método actúa de la siguiente manera:
	      se generan un número fijo de centroides y cada centro tiene una posición
	      aleatoria, una única desviación estándar, una clase y un peso. Para
	      generar una instancia se selecciona un centro al azar, teniendo en cuenta
	      el peso asociado, de tal manera de favorecer a los centros con mayor peso.
	      La clase del ejemplo es determinada por el centroide elegido. El siguiente
	      paso es seleccionar una dirección tal que aleje los valores de atributo
	      del punto central. La dirección es obtenida aleatoriamente, siguiendo una
	      distribución gaussiana cuya desviación estándar es determinada por el
	      centroide elegido.  El resultado, en términos geométricos, es una
	      hiperesfera en donde cada ejemplo rodea un punto central con densidades
	      variables \cite{kirkby_improving_2007}. Este método surge de la necesidad
	      de generar conceptos que no favorezcan a modelos de tipo árbol, tal como
	      sí lo hacía la técnica \acrshort{rtg}.

\end{description}

En lo que refiere a datos multi-etiquetados, las herramientas existentes son más
reducidas. \citeauthor{read_generating_2009} han presentado un marco general
de trabajo para la generación de flujos continuos multi-etiquetados
\cite{read_generating_2009} con el objetivo de generar datos realistas o que
se aproximen a instancias del mundo real. El procedimiento consiste en tomar la
salida obtenida por los algoritmos generadores de datos de etiqueta única y
transformarla en datos multi-etiquetados, y esto lo hacen obedeciendo a
fenómenos, comportamientos o cualidades intrínsecas a las colecciones
multi-etiquetadas del mundo real. La idea es que si estos fenómenos capturados
en datos reales se cumplen en grado similar para los datos sintéticos entonces
se ha logrado generar una colección de datos de valor. Los fenómenos presentados
son los siguientes:

\begin{description} \label{mll_fenomenos}

	\item[Sesgo de etiquetas] Es el fenómeno por el cual existen etiquetas que se
	      presentan en los datos más frecuentemente que otras. A diferencia de
	      colecciones de etiqueta única, puede haber más de una etiqueta por
	      instancia lo que lleva a que este fenómeno se magnifique en colecciones
	      multi-etiquetadas. A su vez, en colecciones de texto es muy común
	      encontrar unas pocas etiquetas que predominan y otras que pertenecen a
	      subconjuntos de instancias muy específicas. Por ejemplo, una etiqueta como
	      \comillas{\texttt{Ficción}} es muy probable que sea relevante en
	      colecciones de cuentos literarios e incluso que figure junto con otras
	      etiquetas, por ejemplo \texttt{\{Ficción, Drama\}}.  No sucede lo mismo
	      con etiquetas tales como \comillas{\texttt{Elefantes}}, que probablemente
	      aparezcan de manera aislada para este conjunto de datos.  Por lo tanto, si
	      se desea generar una colección sintética de texto es posible que se quiera
	      ajustar el valor del parámetro de sesgo de etiquetas para que sea mayor al
	      de otro tipo de colecciones.

	\item[Distribución de etiquetas] La distribución de etiquetas tiene que ver
	      con la forma en que la cardinalidad de etiquetas se distribuye a lo largo
	      de la colección. La cardinalidad de etiquetas, tal como fue formulada en
	      la ecuación~\ref{eq:mll_card}, es la cantidad de etiquetas promedio por
	      instancia y es una de las métricas usadas para conocer el grado de
	      multi-etiquetado de los datos (ver sección~\ref{mll_def_formal}). Observar
	      la distribución de etiquetas es útil para entender la composición de dicha
	      cardinalidad y se calcula tomando el número de veces que se repite cada
	      posible tamaño de subconjuntos de etiquetas. Basándose en este fenómeno se
	      distinguen dos tipos de colecciones, \comillas{A} y \comillas{B}. Las
	      colecciones de tipo A son aquellas donde la cardinalidad de etiquetas es
	      muy cercana a uno, pero resultan ser multi-etiquetadas por la existencia de
	      ejemplos donde el etiquetado único generaría ambigüedad y se resolvió
	      añadiendo etiquetas. Tal es el caso para colecciones de artículos
	      periodísticos \cite{lang_newsweeder_1995} o de imágenes para realizar
	      detección de objetos \cite{boutell_learning_2004}. Por contrapartida, las
	      colecciones de tipo \comillas{B} son aquellas donde existe más de una
	      etiqueta por instancia y suelen situarse en dominios abarcativos. Es el
	      caso para colecciones de funciones genómicas, por ejemplo, en la cual se
	      espera que los genes tengan múltiples funciones
	      \cite{diplaris_protein_2005}.  Otros ejemplos son los de correos
	      electrónicos \cite{hutchison_enron_2004} y conceptos semánticos
	      \cite{snoek_challenge_2006}.

	\item[Relación entre etiquetas] Esta cualidad tiene que ver con el concepto de
	      interdependencia entre etiquetas, descrito en la sección~\ref{intro_mll},
	      y se trata de capturar la aparición mutua de etiquetas en los ejemplos de
	      tal manera de reflejar el grado de dependencia de las etiquetas en el
	      dominio del problema. La idea es que un generador de instancias sintéticas
	      debe ser capaz de asignar subconjuntos de etiquetas, ya no de manera
	      aleatoria, sino respetando esta relación subyacente. Esto se puede lograr
	      diseñando una matriz cuadrada de doble entrada que guarde las
	      probabilidades condicionales entre pares de etiquetas.

	\item[Espacio de atributos] Así como existen relaciones entre etiquetas que
	      los algoritmos explotan para generar datos sintéticos de calidad, también
	      puede analizarse el espacio de atributos y las interdependencias entre sí
	      para optimizar los generadores. El estudio realizado por
	      \citeauthor{read_generating_2009} halló efectos o particularidades
	      frecuentes en los datos: uno de ellos es el efecto
	      \comillas{atributo-etiqueta} por el cual hay atributos que de aparecer en
	      un ejemplo activan una etiqueta. El ejemplo dado surgió en una colección
	      de artículos de noticias tecnológicas \cite{read_classifier_2011}, en
	      donde se observó que los atributos \comillas{\texttt{linux}} y
	      \comillas{\texttt{mobile}} estaban fuertemente emparentados con las
	      etiquetas \comillas{\texttt{Linux}} y \comillas{\texttt{Mobile}},
	      respectivamente. Otro efecto hallado es el denominado
	      \comillas{atributo-combinación} por el cual la aparición de un atributo
	      activa un subconjunto de etiquetas en simultáneo. Por ejemplo, en una
	      colección de noticias \cite{lang_newsweeder_1995} se descubrió que el
	      atributo \comillas{\texttt{arms}} ocurre frecuentemente con las etiquetas
	      \texttt{\{politics.guns, misc.religion\}}. Del mismo modo, existe el
	      \comillas{efecto aleatorio}, el cual engloba a los atributos que no proveen
	      información significativa sobre la presencia de etiquetas o combinaciones
	      de etiquetas. En definitiva, los generadores pueden sacar provecho de este
	      fenómeno otorgando parámetros que permitan ajustar el grado de presencia
	      de los efectos mencionados y así lograr colecciones de datos sintéticos
	      más realistas.

\end{description}
