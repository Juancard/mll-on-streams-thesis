\chapter{Introducción}

\section{Fundamentos}

En los últimos años ha habido un aumento considerable de datos de diversa índole
y generados por fuentes heterogéneas. Según los autores
\citeauthor{gantz_extracting_2011}, el volumen total de datos creados y
replicados en el mundo durante el año 2011 supera los 1.8 ZB (zettabytes) y se
ha estimado que duplica cada dos años \cite{gantz_extracting_2011}. Los avances
en el área de tecnología de la información (IT) han contribuido a una continua
producción de datos y expansión del campo digital, tal es el caso para la red
social \textit{Facebook}, la cual recibe cada hora un flujo de 10 millones de
fotos que publican sus usuarios \cite{mayer-schonberger_big_2013}. A estas
grandes colecciones de datos se las conoce como \textit{big data} y acarrean
nuevas oportunidades y desafíos al campo de las ciencias de la computación. En
cuestiones económicas, un análisis a gran escala en búsqueda de tendencias en el
comportamiento de los usuarios o clientes de un sistema puede dar una ventaja
competitiva en el mercado y, en adición, proveer de un servicio valioso a la
comunidad. Potencialmente, la \textit{big data} puede ser una fuente que
proporcione a la comunidad de conocimiento nuevo sobre el mundo en el que
habita, o como ha mencionado \citeauthor{fayyad_advances_1996} en su escrito
sobre el descubrimiento de conocimiento, “Los datos que percibimos de nuestro
ambiente son la evidencia básica que usamos para construir teorías y modelos
sobre el universo en el que vivimos”\footnote{“Data we capture about our
   environment are the basic evidence we use to build theories and models of the
universe we live in” \cite[p. 2]{fayyad_advances_1996}. Traducción propia.}.

Sin embargo, volúmenes masivos de datos tornan obsoletos los tradicionales
métodos manuales de análisis de datos y surge la necesidad de desarrollar
técnicas automatizadas para extraer patrones en los datos y obtener
conocimiento. Con este fin, se han desarrollado técnicas en las áreas de minería
de datos y aprendizaje de máquinas que abordan estas colecciones en búsqueda de
conocimiento válido y útil. Dichas técnicas se han enfocado en el aprendizaje
por \textit{batch} \cite{gama_knowledge_2010}, lo que significa que el algoritmo
dispone de la colección completa, almacenada en disco, y con la cual genera un
modelo a partir de una o múltiples iteraciones sobre todos los datos. No
obstante, el aprendizaje por \textit{batch} trae aparejada una dificultad en su
misma definición: requiere de todos los datos de la colección presentes y
accesibles en todo momento,  lo cual no siempre es posible. Además se suma una
limitante que es clave en el contexto actual de alta disponibilidad de datos:
hoy en día una buena parte de los datos generados proviene de flujos continuos o
‘\textit{streamings}’ de datos \cite{bifet_big_2014}. Estos flujos son
potencialmente ilimitados, arriban de a una instancia por vez, y son analizados
con restricciones altas de tiempo de procesamiento y de memoria.  Tal es el caso
para aplicaciones de sensores, monitoreo de redes y administración de tráfico,
flujo de clics de un usuario en la web, redes sociales, entre otros.  Los
algoritmos de aprendizaje que actúen en este entorno dinámico deben contar con
mecanismos que permitan manejar cambios en la naturaleza o distribución de los
datos, tanto para incorporar datos nuevos, como para descartar los datos
antiguos. Por estas razones, se torna necesario que las aplicaciones basadas en
clasificación en tiempo real adapten sus operaciones de entrenamiento y
predicción para lograr mejores resultados \cite{sousa_multi-label_2018}.

Dentro del área de minería de datos, una de las principales tareas es la de
clasificación, la cual consiste en entrenar un modelo que sea capaz de asignar
una única etiqueta a una instancia desconocida. No obstante, existen problemas
de clasificación en donde múltiples etiquetas son necesarias para caracterizar
una instancia. Por ejemplo, una noticia de diario referida al accidente aéreo
que sufrió el plantel de fútbol del club Chapecoense puede ser clasificado en la
categoría de “Fútbol” tanto como en la de “Tragedias”. Del mismo modo, un video
documental sobre la vida de Borges puede anotarse como “Biografía”, “Literatura”
o incluso “Buenos Aires” si se mostraran imágenes de la ciudad. Este tipo de
problemas es llamado \acrfull{mll} \footnote{Siglas provenientes de su
abreviación en inglés, Multi-label learning} y representa un nuevo paradigma de
aprendizaje automático, con sus propios retos por afrontar y que aún no ha sido
suficientemente explorado en proyectos de investigación. 

Una clasificación multi-etiqueta permite conocer el grado de correlación entre
una instancia de la colección y una o más etiquetas. Esta cualidad significa un
mayor poder de generalización con respecto a la clasificación tradicional de
única etiqueta, ya que puede abarcar esos mismos problemas y otros de mayor
número de etiquetas. Además existen algoritmos que aprovechan la correlación
entre etiquetas para mejorar la eficiencia de la clasificación y la calidad de
la predicción.

El campo de \acrshort{mll} se ha desarrollado considerablemente en los últimos
años pero hasta el momento muchos de estos trabajos se han llevado a cabo en
ambientes estáticos de aprendizaje por \textit{batch}
\cite{read_classifier_2011}. Por lo que se hace necesario encarar nuevos
proyectos que aborden clasificaciones \acrshort{mll} en contextos de
\textit{streaming} de datos. El desafío entonces consiste en crear
clasificadores que sean capaces de manejar un inmenso número de instancias y
adaptarse al cambio, a la vez que estar preparados para hacer tareas de
predicción en cualquier momento, y todo esto en un contexto de altas
restricciones de tiempo de respuesta y memoria.

\section{Clasificación de flujos de datos multi etiquetados} \todo[inline]{Datos
multietiquetados.} \todo[inline]{Aprendizaje incremental ?}
\todo[inline]{Streamings. Caracteristicas esenciales (potencialmente infinita,
limite de espacio en memoria y de tiempo, etc).} \todo[inline]{Caracteristicas
de una colección multietiquetada (densidad, cardinalidad,etc).}


\section{Motivación} 

Ante la necesidad de hacer frente a un contexto global de generación masiva de
datos y a ritmo acelerado, se hace preciso fortalecer las técnicas de
aprendizaje automático actualmente presentes en el campo. En este escenario ya
no es posible contar con todos los datos almacenados físicamente y la idea de
generar un modelo completo para luego evaluarlo en una fase posterior debe ser
reemplazada por una en donde el modelo esté siempre listo para realizar
predicciones y al mismo tiempo ser capaz de re-entrenarse y recalcular las
métricas de evaluación ante cada nueva instancia abordada. Todo esto en un
contexto cambiante, de alta disponibilidad y limitación en el espacio de
almacenamiento. Si bien existen métodos de clasificación para flujos continuos
que han dado resultados satisfactorios, aún es un campo conveniente de ser
abordado. Asímismo, reproducir los experimentos realizados y fortalecer las
técnicas y herramientas actuales puede ser beneficioso para lograr estudios
precisos y pormenorizados que sean valiosos tanto para la comunidad científica
en sí misma, como también para la sociedad en general.

Por otro lado, si bien existen en el mundo real infinidad de datos multi
etiquetados aún no es posible hallar colecciones disponibles al público que
cuenten con todas las características de un flujo continuo de datos. Uno de los
enfoques abordados es convertir las colecciones existentes en flujos tales que
arriben en conjuntos predefinidos y a lo largo del tiempo. De esta manera los
algoritmos pueden ser utilizados para realizar clasificaciones en un ambiente similar al de un
escenario de \textit{streaming}. Sin embargo, estas colecciones tienen un número
limitado de instancias y por lo tanto no cumplen con la condición de ser
teóricamente infinitos. Es entonces aquí donde surgen las técnicas de generación
sintética de instancias, que buscan reproducir la distribución subyacente de los
datos para simular colecciones de datos del mundo real. La contracara de este
enfoque es que, si bien existen técnicas y herramientas para generar datos
etiquetados, buena parte de ellos son solo aplicables para instancias de una
única etiqueta y los que logran generar datos multi etiquetados no han sido lo
suficientemente explorados en el área. Hasta el momento, los generadores de
instancias multi etiquetadas son capaces de generar datos
cercanos a los de colecciones del mundo real \cite{read_generating_nodate} y
brindan la posibilidad de realizar estudios certeros de algoritmos de
clasificación \cite{read_scalable_2012}. No obstante, debe notarse también que si bien
se han obtenido colecciones sintéticas en sí mismas aún no han logrado generar
instancias para una colección en concreto, respetando sus cualidades
particulares y que las distinguen de otras, tales como la co-ocurrencia de
etiquetas, la densidad y cardinalidad de las etiquetas y la relación entre las
etiquetas y sus \textit{features}, por mencionar algunas. De lograr esta
aproximación se podrá realizar estudios sobre el impacto de los algoritmos sobre
flujos de datos de naturaleza distintiva, o en otras palabras, entender en qué
medida un algoritmo es más apropiado que otro para un conjunto de datos en un
determinado contexto. 

Esta última idea mencionada, es decir, el ser capaz de hallar las fortalezas y
debilidades de un algoritmo de \acrshort{mll} en un contexto determinado es
clave para evaluar la clasificación y entender los resultados obtenidos.
Estudios como el de \citeauthor{sousa_multi-label_2018}
\cite{sousa_multi-label_2018} o el de \citeauthor{read_scalable_2012}
\cite{read_scalable_2012} se han topado con que algoritmos de menor complejidad
pueden ser competitivos o incluso superar las métricas de otros algoritmos más
complejos. Esta variabilidad en los resultados no solo contrae la necesidad
antes mencionada de realizar más estudios al respecto, sino que también abre las
puertas a incursionar en soluciones de ensambles de algoritmos. Estos ensambles
han dado probada muestra de potenciarce ante la diversidad de resultados
obtenidos por sus estimadores base \cite{polikar_polikar_2006}, ya que son
capaces de disminuir el error total mediante estrategias combinativas. De
cualquier manera, las estrategias de ensamble existentes para flujos continuos
no han recibido la misma atención que aquellas aplicadas sobre ambientes de
\textit{batch} y queda mucho camino por recorrer.



\section{Objetivos}

objetivos son: 
- Generar 

\section{Aportes}

El presente  trabajo de investigación aborda el campo de aprendizaje por
multi-etiquetas a partir de la experimentación y evaluación de técnicas y
algoritmos de la literatura sobre colecciones de naturaleza cambiante.
\acrshort{mll} es un paradigma emergente de aprendizaje supervisado cuyas
características implícitas abren paso a nuevos desafíos que derivan del
crecimiento exponencial de etiquetas y sus combinaciones, y del costo
computacional de entrenar y consultar el modelo. También suelen presentarse
otras propiedades como la alta dimensionalidad, data evolutiva y desbalanceada o
dependencia entre etiquetas, las cuales implican una re-significación de las
técnicas y métodos tradicionales del área de minería de datos.

El paradigma de \acrshort{mll} ha dado muestras de su eficiencia en términos de
tiempos de configuración y ejecución de las tareas, bajo diversos campos de
aplicación tales como los de categorización de texto, diagnósticos médicos,
minería de redes sociales o análisis de datos químicos, y se mantiene en
constante expansión hacia nuevos dominios de aplicación. Asimismo, su continua
integración a problemas de diversa naturaleza ha contribuido a alimentar esta
tendencia.

La tarea de clasificación de \textit{streaming} de datos se enfoca en problemas
donde objetos del mundo real son generados y procesados en tiempo real. Datos de
este tipo, y que además poseen múltiples etiquetas, son frecuentes en escenarios
del mundo real tal como sitios de publicación de imágenes, correos electrónicos
o portales de noticias. Abordar este tipo de problemas implica que los
algoritmos sean capaces de identificar cambios de concepto en los datos y
adaptarse al nuevo contexto. De lograr esto, se podrá generar modelos más
sólidos, ya que se cuenta con un mayor número de objetos, y que se encontrarán
aptos para predecir en cualquier momento. Surge entonces el reto de crear
clasificadores que actúen en ambientes de altas restricciones computacionales y
sean capaces de manejar un inmenso número de instancias, lidiar con evoluciones
en los datos y estar listos para resolver tareas de predicción en tiempo real.

A diferencia de otros trabajos de investigación recientes, este proyecto lleva a
cabo estudios experimentales sobre el tema de clasificaciones multi-etiquetas,
para hallar las fortalezas y debilidades de distintos algoritmos de aprendizaje
sobre distintos tipos de colecciones, con miras a aportar de un mayor
conocimiento empírico sobre el tema a la comunidad científica especializada en
tareas de clasificación de flujos de datos multi-etiquetados. El presente
trabajo espera contribuir al estudio de métodos y técnicas asentadas del campo,
pero también examinar algoritmos exitosos del campo de aprendizaje por
\textit{batch}, particularmente los de ensambles de estimadores, a fin de
extender su funcionalidad a ambientes de flujos continuos, analizar su desempeño
y determinar en qué medida son aptos o no para este tipo de ambientes.


\section{Organización del Trabajo} 

\todo[inline]{Describir las distintas secciones del trabajo}
