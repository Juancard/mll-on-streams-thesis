
@inproceedings{karponi_empirical_2017,
	location = {Cham},
	title = {An Empirical Comparison of Methods for Multi-label Data Stream Classification},
	isbn = {978-3-319-47898-2},
	doi = {10.1007/978-3-319-47898-2_16},
	series = {Advances in Intelligent Systems and Computing},
	abstract = {This paper studies the problem of multi-label classification in the context of data streams. We discuss related work in this area and present our implementation of several existing approaches as part of the Mulan software. We present empirical results on a real-world data stream concerning media monitoring and discuss and draw a number of conclusions regarding their performance.},
	pages = {151--159},
	booktitle = {Advances in Big Data},
	publisher = {Springer International Publishing},
	author = {Karponi, Konstantina and Tsoumakas, Grigorios},
	editor = {Angelov, Plamen and Manolopoulos, Yannis and Iliadis, Lazaros and Roy, Asim and Vellasco, Marley},
	date = {2017},
	langid = {english},
	keywords = {Multi-label learning, Classification, Data streams, Media monitoring}
}

@article{read_generating_nodate,
	title = {Generating Synthetic Multi-label Data Streams},
	abstract = {There are many available methods for generating synthetic data streams. Such methods have been justiﬁed by the need to study the eﬃcacy of algorithms on a theoretically inﬁnite stream, and also a lack of real-world data of suﬃcient size. Although multi-label classiﬁcation has attracted considerable interest in recent years, most of this work has been carried out in the context of a batch learning environment rather than a data stream. This paper makes an in-depth analysis of multi-label data, and presents a general framework for generating synthetic multi-label data streams.},
	pages = {16},
	author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoﬀ},
	langid = {english}
}

@article{nguyen_multi-label_2019,
	title = {Multi-label classification via label correlation and first order feature dependance in a data stream},
	volume = {90},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320319300123},
	doi = {10.1016/j.patcog.2019.01.007},
	abstract = {Many batch learning algorithms have been introduced for offline multi-label classification ({MLC}) over the years. However, the increasing data volume in many applications such as social networks, sensor networks, and traffic monitoring has posed many challenges to batch {MLC} learning. For example, it is often expensive to re-train the model with the newly arrived samples, or it is impractical to learn on the large volume of data at once. The research on incremental learning is therefore applicable to a large volume of data and especially for data stream. In this study, we develop a Bayesian-based method for learning from multi-label data streams by taking into consideration the correlation between pairs of labels and the relationship between label and feature. In our model, not only the label correlation is learned with each arrived sample with ground truth labels but also the number of predicted labels are adjusted based on Hoeffding inequality and the label cardinality. We also extend the model to handle missing values, a problem common in many real-world data. To handle concept drift, we propose a decay mechanism focusing on the age of the arrived samples to incrementally adapt to the change of data. The experimental results show that our method is highly competitive compared to several well-known benchmark algorithms under both the stationary and concept drift settings.},
	pages = {35--51},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Nguyen, Tien Thanh and Nguyen, Thi Thu Thuy and Luong, Anh Vu and Nguyen, Quoc Viet Hung and Liew, Alan Wee-Chung and Stantic, Bela},
	urldate = {2020-06-25},
	date = {2019-06-01},
	langid = {english},
	keywords = {20ng, Concept drift, enron, Multi-label classification, Data stream, Feature dependence, Label correlation, Multi-label learning, Online learning}
}

@article{trohidis_multi-label_2011,
	title = {Multi-label classification of music by emotion},
	volume = {2011},
	issn = {1687-4722},
	url = {https://doi.org/10.1186/1687-4722-2011-426793},
	doi = {10.1186/1687-4722-2011-426793},
	abstract = {This work studies the task of automatic emotion detection in music. Music may evoke more than one different emotion at the same time. Single-label classification and regression cannot model this multiplicity. Therefore, this work focuses on multi-label classification approaches, where a piece of music may simultaneously belong to more than one class. Seven algorithms are experimentally compared for this task. Furthermore, the predictive power of several audio features is evaluated using a new multi-label feature selection method. Experiments are conducted on a set of 593 songs with six clusters of emotions based on the Tellegen-Watson-Clark model of affect. Results show that multi-label modeling is successful and provide interesting insights into the predictive quality of the algorithms and features.},
	pages = {4},
	number = {1},
	journaltitle = {{EURASIP} Journal on Audio, Speech, and Music Processing},
	shortjournal = {J {AUDIO} {SPEECH} {MUSIC} {PROC}.},
	author = {Trohidis, Konstantinos and Tsoumakas, Grigorios and Kalliris, George and Vlahavas, Ioannis},
	urldate = {2020-06-24},
	date = {2011-09-18},
	langid = {english},
	keywords = {evaluation, datasets, br, lc, music, rakel}
}

@online{noauthor_notitle_nodate,
	url = {https://pdfs.semanticscholar.org/0caf/7892917a53011e062c9f9057551a601b6695.pdf},
	urldate = {2020-06-24}
}

@article{read_meka_2016,
	title = {{MEKA}: A multi-label/multi-target extension to {WEKA}},
	volume = {17},
	issn = {1533-7928},
	url = {https://researchcommons.waikato.ac.nz/handle/10289/10136},
	shorttitle = {{MEKA}},
	abstract = {Multi-label classification has rapidly attracted interest in the machine learning literature, and there are now a large number and considerable variety of methods for this type of learning. We present {MEKA}: an open-source Java framework based on the well-known {WEKA} library. {MEKA} provides interfaces to facilitate practical application, and a wealth of multi-label classifiers, evaluation metrics, and tools for multi-label experiments and development. It supports multi-label and multi-target data, including in incremental and semi- supervised contexts.},
	pages = {1--5},
	number = {21},
	author = {Read, Jesse and Reutemann, Peter and Pfahringer, Bernhard and Holmes, Geoffrey},
	urldate = {2020-06-24},
	date = {2016},
	langid = {english},
	note = {Accepted: 2016-04-26T04:04:07Z},
	keywords = {meka}
}

@book{bifet_machine_2018,
	title = {Machine Learning for Data Streams with Practical Examples in {MOA}},
	isbn = {978-0-262-03779-2},
	abstract = {Today many information sources—including sensor networks, financial markets, social networks, and healthcare monitoring—are so-called data streams, arriving sequentially and at high speed. Analysis must take place in real time, with partial data and without the capacity to store the entire data set. This book presents algorithms and techniques used in data stream mining and real-time analytics. Taking a hands-on approach, the book demonstrates the techniques using {MOA} (Massive Online Analysis), a popular, freely available open-source software framework, allowing readers to try out the techniques after reading the explanations.

The book first offers a brief introduction to the topic, covering big data mining, basic methodologies for mining data streams, and a simple example of {MOA}. More detailed discussions follow, with chapters on sketching techniques, change, classification, ensemble methods, regression, clustering, and frequent pattern mining. Most of these chapters include exercises, an {MOA}-based lab session, or both. Finally, the book discusses the {MOA} software, covering the {MOA} graphical user interface, the command line, use of its {API}, and the development of new methods within {MOA}. The book will be an essential reference for readers who want to use data stream mining as a tool, researchers in innovation or data stream mining, and programmers who want to create new algorithms for {MOA}.

https://moa.cms.waikato.ac.nz/book/},
	author = {Bifet, Albert and Gavaldà, Ricard and Pfahringer, Bernhard and Holmes, Geoffrey},
	date = {2018-03-02}
}

@article{lang_newsweeder_nodate,
	title = {{NewsWeeder}: Learning to Filter Netnews (To appear in {ML} 95)},
	abstract = {A significant problem in many information filtering systems is the dependence on the user for the creation and maintenance of a user profile, which describes the user’s interests. {NewsWeeder} is a netnews-filtering system that addresses this problem by letting the user rate his or her interest level for each article being read (1-5), and then learning a user profile based on these ratings. This paper describes how {NewsWeeder} accomplishes this task, and examines the alternative learning methods used. The results show that a learning algorithm based on the Minimum Description Length ({MDL}) principle was able to raise the percentage of interesting articles to be shown to users from 14\% to 52\% on average. Further, this performance significantly outperformed (by 21\%) one of the most successful techniques in Information Retrieval ({IR}), termfrequency/inverse-document-frequency (tf-idf) weighting.},
	pages = {9},
	author = {Lang, Ken},
	langid = {english},
	keywords = {20ng}
}

@incollection{hutchison_enron_2004,
	location = {Berlin, Heidelberg},
	title = {The Enron Corpus: A New Dataset for Email Classification Research},
	volume = {3201},
	isbn = {978-3-540-23105-9 978-3-540-30115-8},
	url = {http://link.springer.com/10.1007/978-3-540-30115-8_22},
	shorttitle = {The Enron Corpus},
	abstract = {Automated classiﬁcation of email messages into user-speciﬁc folders and information extraction from chronologically ordered email streams have become interesting areas in text learning research. However, the lack of large benchmark collections has been an obstacle for studying the problems and evaluating the solutions. In this paper, we introduce the Enron corpus as a new test bed. We analyze its suitability with respect to email folder prediction, and provide the baseline results of a stateof-the-art classiﬁer (Support Vector Machines) under various conditions, including the cases of using individual sections (From, To, Subject and body) alone as the input to the classiﬁer, and using all the sections in combination with regression weights.},
	pages = {217--226},
	booktitle = {Machine Learning: {ECML} 2004},
	publisher = {Springer Berlin Heidelberg},
	author = {Klimt, Bryan and Yang, Yiming},
	editor = {Boulicaut, Jean-François and Esposito, Floriana and Giannotti, Fosca and Pedreschi, Dino},
	editorb = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
	editorbtype = {redactor},
	urldate = {2020-06-15},
	date = {2004},
	langid = {english},
	doi = {10.1007/978-3-540-30115-8_22},
	note = {Series Title: Lecture Notes in Computer Science},
	keywords = {enron, datasets}
}

@inproceedings{snoek_challenge_2006,
	location = {Santa Barbara, {CA}, {USA}},
	title = {The challenge problem for automated detection of 101 semantic concepts in multimedia},
	isbn = {978-1-59593-447-5},
	url = {http://portal.acm.org/citation.cfm?doid=1180639.1180727},
	doi = {10.1145/1180639.1180727},
	abstract = {We introduce the challenge problem for generic video indexing to gain insight in intermediate steps that aﬀect performance of multimedia analysis methods, while at the same time fostering repeatability of experiments. To arrive at a challenge problem, we provide a general scheme for the systematic examination of automated concept detection methods, by decomposing the generic video indexing problem into 2 unimodal analysis experiments, 2 multimodal analysis experiments, and 1 combined analysis experiment. For each experiment, we evaluate generic video indexing performance on 85 hours of international broadcast news data, from the {TRECVID} 2005/2006 benchmark, using a lexicon of 101 semantic concepts. By establishing a minimum performance on each experiment, the challenge problem allows for component-based optimization of the generic indexing issue, while simultaneously oﬀering other researchers a reference for comparison during indexing methodology development. To stimulate further investigations in intermediate analysis steps that inﬂuence video indexing performance, the challenge oﬀers to the research community a manually annotated concept lexicon, pre-computed low-level multimedia features, trained classiﬁer models, and ﬁve experiments together with baseline performance, which are all available at http://www.mediamill.nl/challenge/.},
	eventtitle = {the 14th annual {ACM} international conference},
	pages = {421},
	booktitle = {Proceedings of the 14th annual {ACM} international conference on Multimedia  - {MULTIMEDIA} '06},
	publisher = {{ACM} Press},
	author = {Snoek, Cees G. M. and Worring, Marcel and van Gemert, Jan C. and Geusebroek, Jan-Mark and Smeulders, Arnold W. M.},
	urldate = {2020-06-15},
	date = {2006},
	langid = {english},
	keywords = {mediamill, datasets}
}

@article{montiel_scikit-multiow_nodate,
	title = {Scikit-Multiﬂow: A Multi-output Streaming Framework},
	pages = {5},
	author = {Montiel, Jacob},
	langid = {english},
	keywords = {Multi-label classification, streaming, scikit-multiflow}
}

@article{maruthupandi_multi-label_2017,
	title = {Multi-label text classification using optimised feature sets},
	volume = {9},
	abstract = {Multi-label text classification is the process of assigning multi-labels to an instance. A significant aspect of the text classification problem is the high dimensionality of the data which hinders the performance of the classifier. Hence, feature selection plays a significant role in classification process that removes the irrelevant data. In this paper, wrapper-based hybrid artificial bee colony and bacterial foraging optimisation ({HABBFO}) approach has been proposed to select the most appropriate feature subset for prediction. Initially, pre-processing such as tokenisation, stop word removal and stemming has been performed to extract the features (words). Experiments are conducted on the benchmark dataset and the results show that the proposed approach achieves better performance compared to the other feature selection techniques.},
	pages = {237--248},
	journaltitle = {International Journal of Data Mining, Modelling and Management ({IJDMMM})},
	author = {Maruthupandi, J and Vimala Devi, K},
	date = {2017-09}
}

@inproceedings{zhang_multi-label_2010,
	title = {Multi-label learning by exploiting label dependency},
	booktitle = {Proceedings of the 16th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '10},
	author = {Zhang, Min-Ling and Zhang, Kun},
	date = {2010}
}

@article{read_scalable_2012,
	title = {Scalable and efficient multi-label classification for evolving data streams},
	volume = {88},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-012-5279-6},
	doi = {10.1007/s10994-012-5279-6},
	abstract = {Many challenging real world problems involve multi-label data streams. Efficient methods exist for multi-label classification in non-streaming scenarios. However, learning in evolving streaming scenarios is more challenging, as classifiers must be able to deal with huge numbers of examples and to adapt to change using limited time and memory while being ready to predict at any point.},
	pages = {243--272},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Read, Jesse and Bifet, Albert and Holmes, Geoff and Pfahringer, Bernhard},
	urldate = {2020-06-17},
	date = {2012-07-01},
	langid = {english}
}

@online{noauthor_multilabel_nodate,
	title = {Multilabel Classification - Problem Analysis, Metrics and Techniques {\textbar} Francisco Herrera {\textbar} Springer},
	url = {https://www.springer.com/gp/book/9783319411101},
	urldate = {2020-06-15}
}

@article{sousa_multi-label_2018,
	title = {Multi-label classification from high-speed data streams with adaptive model rules and random rules},
	journaltitle = {Progress in Artificial Intelligence},
	author = {Sousa, Ricardo and Gama, João},
	date = {2018}
}

@inproceedings{hulten_mining_2001,
	location = {San Francisco, California},
	title = {Mining Time-changing Data Streams},
	series = {{KDD} '01},
	pages = {97--106},
	booktitle = {Proceedings of the Seventh {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {{ACM}},
	author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
	date = {2001}
}

@book{gama_knowledge_2010,
	title = {Knowledge Discovery from Data Streams},
	author = {Gama, João},
	date = {2010}
}

@inproceedings{read_journal_2011,
	title = {Journal of Machine Learning Research - Proceedings Track},
	pages = {19--25},
	booktitle = {Streaming Multi-label Classification},
	author = {Read, Jesse and Bifet, Albert and Holmes, Geoffrey and Pfahringer, Bernhard},
	date = {2011}
}

@article{osojnik_multi-label_2017,
	title = {Multi-label classification via multi-target regression on data streams},
	volume = {106},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-016-5613-5},
	doi = {10.1007/s10994-016-5613-5},
	abstract = {Multi-label classification ({MLC}) tasks are encountered more and more frequently in machine learning applications. While {MLC} methods exist for the classical batch setting, only a few methods are available for streaming setting. In this paper, we propose a new methodology for {MLC} via multi-target regression in a streaming setting. Moreover, we develop a streaming multi-target regressor {iSOUP}-Tree that uses this approach. We experimentally compare two variants of the {iSOUP}-Tree method (building regression and model trees), as well as ensembles of {iSOUP}-Trees with state-of-the-art tree and ensemble methods for {MLC} on data streams. We evaluate these methods on a variety of measures of predictive performance (appropriate for the {MLC} task). The ensembles of {iSOUP}-Trees perform significantly better on some of these measures, especially the ones based on label ranking, and are not significantly worse than the competitors on any of the remaining measures. We identify the thresholding problem for the task of {MLC} on data streams as a key issue that needs to be addressed in order to obtain even better results in terms of predictive performance.},
	pages = {745--770},
	number = {6},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Osojnik, Aljaž and Panov, Panče and Džeroski, Sašo},
	urldate = {2020-06-15},
	date = {2017-06-01},
	langid = {english},
	keywords = {20ng, enron, evaluation, Multi-label classification, streaming, datasets, preliminares}
}

@inproceedings{read_multi-label_2008,
	title = {Multi-label Classification Using Ensembles of Pruned Sets},
	booktitle = {2008 Eighth {IEEE} International Conference on Data Mining},
	author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff},
	date = {2008}
}

@article{tsoumakas_multi-label_2007,
	title = {Multi-Label Classification},
	volume = {3},
	pages = {1--13},
	number = {3},
	journaltitle = {Int. J. Data Warehouse. Min.},
	author = {Tsoumakas, Grigorios and Katakis, Ioannis},
	date = {2007}
}

@article{gantz_extracting_2011,
	title = {Extracting value from chaos},
	pages = {1--12},
	journaltitle = {{IDC} {IView}},
	author = {Gantz, J and Reinsel, D},
	date = {2011}
}

@inproceedings{chen_extracting_2009,
	location = {Paris, France},
	title = {Extracting discriminative concepts for domain adaptation in text mining},
	isbn = {978-1-60558-495-9},
	url = {http://portal.acm.org/citation.cfm?doid=1557019.1557045},
	doi = {10.1145/1557019.1557045},
	eventtitle = {the 15th {ACM} {SIGKDD} international conference},
	pages = {179},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '09},
	publisher = {{ACM} Press},
	author = {Chen, Bo and Lam, Wai and Tsang, Ivor and Wong, Tak-Lam},
	urldate = {2020-03-01},
	date = {2009},
	langid = {english},
	keywords = {20ng, datasets}
}

@inproceedings{liu_deep_2017,
	location = {Shinjuku Tokyo Japan},
	title = {Deep Learning for Extreme Multi-label Text Classification},
	isbn = {978-1-4503-5022-8},
	url = {https://dl.acm.org/doi/10.1145/3077136.3080834},
	doi = {10.1145/3077136.3080834},
	abstract = {Extreme multi-label text classi cation ({XMTC}) refers to the problem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. e huge label space raises research challenges such as data sparsity and scalability. Signi cant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for {XMTC}, despite its big successes in other related areas. is paper presents the rst a empt at applying deep learning to {XMTC}, with a family of new Convolutional Neural Network ({CNN}) models which are tailored for multi-label classi cation in particular. With a comparative evaluation of 7 state-of-the-art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed {CNN} approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in particular, it outperformed the second best method by 11.7\% ∼ 15.3\% in precision@K and by 11.5\% ∼ 11.7\% in {NDCG}@K for K = 1,3,5.},
	eventtitle = {{SIGIR} '17: The 40th International {ACM} {SIGIR} conference on research and development in Information Retrieval},
	pages = {115--124},
	booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {{ACM}},
	author = {Liu, Jingzhou and Chang, Wei-Cheng and Wu, Yuexin and Yang, Yiming},
	urldate = {2020-06-15},
	date = {2017-08-07},
	langid = {english},
	keywords = {mediamill, Multi-label classification, cnn-kim}
}

@article{read_classifier_2011,
	title = {Classifier chains for multi-label classification},
	volume = {85},
	pages = {333--359},
	number = {3},
	journaltitle = {Mach. Learn.},
	author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe},
	date = {2011}
}

@inproceedings{gargiulo_deep_2018,
	title = {Deep Convolution Neural Network for Extreme Multi-label Text Classification},
	booktitle = {Proceedings of the 11th International Joint Conference on Biomedical Engineering Systems and Technologies},
	author = {Gargiulo, Francesco and Silvestri, Stefano and Ciampi, Mario},
	date = {2018}
}

@article{pereira_categorizing_2016,
	title = {Categorizing feature selection methods for multi-label classification},
	volume = {49},
	pages = {57--78},
	number = {1},
	journaltitle = {Artificial Intelligence Review},
	author = {Pereira, Rafael B and Plastino, Alexandre and Zadrozny, Bianca and Merschmann, Luiz H C},
	date = {2016}
}

@article{chen_big_2014,
	title = {Big data: A survey},
	volume = {19},
	author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
	date = {2014}
}

@book{mayer-schonberger_big_2013,
	title = {Big Data: A Revolution that Will Transform how We Live, Work, and Think},
	series = {An Eamon Dolan book},
	publisher = {Houghton Mifflin Harcourt},
	author = {Mayer-Schonberger, V and Cukier, K},
	date = {2013}
}

@inproceedings{bifet_big_2014,
	title = {Big Data Stream Learning with {SAMOA}},
	booktitle = {2014 {IEEE} International Conference on Data Mining Workshop},
	author = {Bifet, Albert and De Francisci Morales, Gianmarco},
	date = {2014}
}

@article{madjarov_extensive_2012,
	title = {An extensive experimental comparison of methods for multi-label learning},
	volume = {45},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320312001203},
	doi = {10.1016/j.patcog.2012.03.004},
	abstract = {Multi-label learning has received signiﬁcant attention in the research community over the past few years: this has resulted in the development of a variety of multi-label learning methods. In this paper, we present an extensive experimental comparison of 12 multi-label learning methods using 16 evaluation measures over 11 benchmark datasets. We selected the competing methods based on their previous usage by the community, the representation of different groups of methods and the variety of basic underlying machine learning methods. Similarly, we selected the evaluation measures to be able to assess the behavior of the methods from a variety of view-points. In order to make conclusions independent from the application domain, we use 11 datasets from different domains. Furthermore, we compare the methods by their efﬁciency in terms of time needed to learn a classiﬁer and time needed to produce a prediction for an unseen example. We analyze the results from the experiments using Friedman and Nemenyi tests for assessing the statistical signiﬁcance of differences in performance. The results of the analysis show that for multi-label classiﬁcation the best performing methods overall are random forests of predictive clustering trees ({RF}-{PCT}) and hierarchy of multi-label classiﬁers ({HOMER}), followed by binary relevance ({BR}) and classiﬁer chains ({CC}). Furthermore, {RF}-{PCT} exhibited the best performance according to all measures for multi-label ranking. The recommendation from this study is that when new methods for multi-label learning are proposed, they should be compared to {RF}-{PCT} and {HOMER} using multiple evaluation measures.},
	pages = {3084--3104},
	number = {9},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Madjarov, Gjorgji and Kocev, Dragi and Gjorgjevikj, Dejan and Džeroski, Sašo},
	urldate = {2020-06-15},
	date = {2012-09},
	langid = {english},
	keywords = {enron, evaluation, mediamill, Multi-label classification, datasets}
}

@book{fayyad_advances_1996,
	location = {Menlo Park, {CA}, {USA}},
	title = {Advances in Knowledge Discovery and Data Mining},
	pagetotal = {1–34},
	publisher = {American Association for Artificial Intelligence},
	author = {Fayyad, Usama M and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
	editor = {{Fayyad, Usama M. and Piatetsky-Shapiro, Gregory and Smyth, Padhraic and Uthurusamy, Ramasamy}},
	date = {1996},
	note = {Section: From data mining to knowledge discovery: an overview}
}

@article{gibaja_tutorial_2015,
	title = {A Tutorial on Multi-Label Learning},
	volume = {47},
	abstract = {Multi-label learning has become a relevant learning paradigm in the last years due to the increasing number of fields where it can be applied and also to the emerging number of techniques that are being developed. This paper presents an up-to-date tutorial about multi-label learning that introduces the paradigm and describes the main contributions developed. Evaluation measures, fields of application, trending topics and resources are also presented.},
	journaltitle = {{ACM} Computing Surveys},
	author = {Gibaja, Eva and Ventura, Sebastian},
	date = {2015}
}

@article{tanaka_multi-label_2015,
	title = {A multi-label approach using binary relevance and decision trees applied to functional genomics},
	volume = {54},
	abstract = {Many classification problems, especially in the field of bioinformatics, are associated with more than one class, known as multi-label classification problems. In this study, we propose a new adaptation for the Binary Relevance algorithm taking into account possible relations among labels, focusing on the interpretability of the model, not only on its performance. Experiments were conducted to compare the performance of our approach against others commonly found in the literature and applied to functional genomic datasets. The experimental results show that our proposal has a performance comparable to that of other methods and that, at the same time, it provides an interpretable model from the multi-label problem.},
	pages = {85--95},
	journaltitle = {J. Biomed. Inform.},
	author = {Tanaka, Erica Akemi and Nozawa, Sérgio Ricardo and Macedo, Alessandra Alaniz and Baranauskas, José Augusto},
	date = {2015},
	langid = {english},
	keywords = {Multi-label classification, Decision tree, Functional genomics}
}

@inproceedings{goncalves_genetic_2013,
	title = {A Genetic Algorithm for Optimizing the Label Ordering in Multi-label Classifier Chains},
	booktitle = {2013 {IEEE} 25th International Conference on Tools with Artificial Intelligence},
	author = {Goncalves, Eduardo Correa and Plastino, Alexandre and Freitas, Alex A},
	date = {2013}
}

@article{zhang_review_2014,
	title = {A Review On Multi-Label Learning Algorithms},
	volume = {26},
	pages = {1819--1837},
	journaltitle = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
	date = {2014}
}

@article{sousa_multi-label_2018-1,
	title = {Multi-label classification from high-speed data streams with adaptive model rules and random rules},
	journaltitle = {Progress in Artificial Intelligence},
	author = {Sousa, Ricardo and Gama, João},
	date = {2018}
}

@article{dembczynski_bayes_nodate,
	title = {Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains},
	abstract = {In the realm of multilabel classiﬁcation ({MLC}), it has become an opinio communis that optimal predictive performance can only be achieved by learners that explicitly take label dependence into account. The goal of this paper is to elaborate on this postulate in a critical way. To this end, we formalize and analyze {MLC} within a probabilistic setting. Thus, it becomes possible to look at the problem from the point of view of risk minimization and Bayes optimal prediction. Moreover, inspired by our probabilistic setting, we propose a new method for {MLC} that generalizes and outperforms another approach, called classiﬁer chains, that was recently introduced in the literature.},
	pages = {8},
	author = {Dembczynski, Krzysztof and Cheng, Weiwei and Hüllermeier, Eyke},
	langid = {english},
	keywords = {model, pcc}
}

@article{polikar_polikar_2006,
	title = {Polikar, R.: Ensemble based systems in decision making. {IEEE} Circuit Syst. Mag. 6, 21-45},
	volume = {6},
	doi = {10.1109/MCAS.2006.1688199},
	shorttitle = {Polikar, R.},
	abstract = {In matters of great importance that have financial, medical, social, or other implications, we often seek a second opinion before making a decision, sometimes a third, and sometimes many more. In doing so, we weigh the individual opinions, and combine them through some thought process to reach a final decision that is presumably the most informed one. The process of consulting "several experts" before making a final decision is perhaps second nature to us; yet, the extensive benefits of such a process in automated decision making applications have only recently been discovered by computational intelligence community. Also known under various other names, such as multiple classifier systems, committee of classifiers, or mixture of experts, ensemble based systems have shown to produce favorable results compared to those of single-expert systems for a broad range of applications and under a variety of scenarios. Design, implementation and application of such systems are the main topics of this article. Specifically, this paper reviews conditions under which ensemble based systems may be more beneficial than their single classifier counterparts, algorithms for generating individual components of the ensemble systems, and various procedures through which the individual classifiers can be combined. We discuss popular ensemble based algorithms, such as bagging, boosting, {AdaBoost}, stacked generalization, and hierarchical mixture of experts; as well as commonly used combination rules, including algebraic combination of outputs, voting based techniques, behavior knowledge space, and decision templates. Finally, we look at current and future research directions for novel applications of ensemble systems. Such applications include incremental learning, data fusion, feature selection, learning with missing features, confidence estimation, and error correcting output codes; all areas in which ensemble systems have shown great promise},
	pages = {21--45},
	journaltitle = {Circuits and Systems Magazine, {IEEE}},
	shortjournal = {Circuits and Systems Magazine, {IEEE}},
	author = {Polikar, Robi},
	date = {2006-10-06}
}

@article{zheng_survey_2020,
	title = {A Survey on Multi-Label Data Stream Classification},
	volume = {8},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2962059},
	abstract = {Nowadays, many real-world applications of our daily life generate massive volume of streaming data at a higher speed than ever before, to name a few, Web clicking data streams, sensor network data and credit transaction streams. Contrary to traditional data mining using static datasets, there are several challenges for data stream mining, for instance, finite memory, one-pass and timely reaction. In this survey, we provide a comprehensive review of existing multi-label streams mining algorithms and categorize these methods based on different perspectives, which mainly focus on the multi-label data stream classification. We first briefly summarize existing multi-label and data stream classification algorithms and discuss their merits and demerits. Secondly, we identify mining constraints on classification for multi-label streaming data, and present a comprehensive study in algorithms for multi-label data stream classification. Finally, several challenges and open issues in multi-label data stream classification are discussed, which are worthwhile to be pursued by the researchers in the future.},
	pages = {1249--1275},
	journaltitle = {{IEEE} Access},
	author = {Zheng, X. and Li, P. and Chu, Z. and Hu, X.},
	date = {2020},
	note = {Conference Name: {IEEE} Access},
	keywords = {Classification algorithms, data mining, Data mining, Data models, data stream mining, Data stream mining, Decision trees, multi-label classification, multi-label data, multilabel data stream classification, multilabel streams mining algorithms, pattern classification, Prediction algorithms, Streaming media, Vegetation, Web clicking data streams}
}

@inproceedings{wang_weighted_2017,
	location = {Cham},
	title = {Weighted Ensemble Classification of Multi-label Data Streams},
	isbn = {978-3-319-57529-2},
	doi = {10.1007/978-3-319-57529-2_43},
	series = {Lecture Notes in Computer Science},
	abstract = {Many real world applications involve classification of multi-label data streams. However, most existing classification models mostly focused on classifying single-label data streams. Learning in multi-label data stream scenarios is more challenging, as the classification systems should be able to consider several properties, such as large data volumes, label correlations and concept drifts. In this paper, we propose an efficient and effective ensemble model for multi-label stream classification based on {ML}-{KNN} (Multi-Label {KNN}) [31] and propose a balance {AdjustWeight} function to combine the predictions which can efficiently process high-speed multi-label stream data with concept drifts. The empirical results indicate that our approach achieves a high accuracy and low storage cost, and outperforms the existing methods {ML}-{KNN} and {SMART} [14].},
	pages = {551--562},
	booktitle = {Advances in Knowledge Discovery and Data Mining},
	publisher = {Springer International Publishing},
	author = {Wang, Lulu and Shen, Hong and Tian, Hui},
	editor = {Kim, Jinho and Shim, Kyuseok and Cao, Longbing and Lee, Jae-Gil and Lin, Xuemin and Moon, Yang-Sae},
	date = {2017},
	langid = {english},
	keywords = {Data stream, Classification, Multi-label, preliminares}
}

@article{nguyen_multi-label_2019-1,
	title = {Multi-label classification via incremental clustering on an evolving data stream},
	volume = {95},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320319302328},
	doi = {10.1016/j.patcog.2019.06.001},
	pages = {96--113},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Nguyen, Tien Thanh and Dang, Manh Truong and Luong, Anh Vu and Liew, Alan Wee-Chung and Liang, Tiancai and {McCall}, John},
	urldate = {2021-01-09},
	date = {2019-11},
	langid = {english},
	keywords = {preliminares}
}

@article{wickramasinghe_naive_2020,
	title = {Naive Bayes: applications, variations and vulnerabilities: a review of literature with code snippets for implementation},
	issn = {1432-7643, 1433-7479},
	url = {http://link.springer.com/10.1007/s00500-020-05297-6},
	doi = {10.1007/s00500-020-05297-6},
	shorttitle = {Naive Bayes},
	journaltitle = {Soft Computing},
	shortjournal = {Soft Comput},
	author = {Wickramasinghe, Indika and Kalutarage, Harsha},
	urldate = {2021-01-11},
	date = {2020-09-09},
	langid = {english}
}

@article{arar_feature_2017,
	title = {A feature dependent Naive Bayes approach and its application to the software defect prediction problem},
	volume = {59},
	issn = {1568-4946},
	url = {http://www.sciencedirect.com/science/article/pii/S1568494617303083},
	doi = {10.1016/j.asoc.2017.05.043},
	abstract = {Naive Bayes is one of the most widely used algorithms in classification problems because of its simplicity, effectiveness, and robustness. It is suitable for many learning scenarios, such as image classification, fraud detection, web mining, and text classification. Naive Bayes is a probabilistic approach based on assumptions that features are independent of each other and that their weights are equally important. However, in practice, features may be interrelated. In that case, such assumptions may cause a dramatic decrease in performance. In this study, by following preprocessing steps, a Feature Dependent Naive Bayes ({FDNB}) classification method is proposed. Features are included for calculation as pairs to create dependence between one another. This method was applied to the software defect prediction problem and experiments were carried out using widely recognized {NASA} {PROMISE} data sets. The obtained results show that this new method is more successful than the standard Naive Bayes approach and that it has a competitive performance with other feature-weighting techniques. A further aim of this study is to demonstrate that to be reliable, a learning model must be constructed by using only training data, as otherwise misleading results arise from the use of the entire data set.},
	pages = {197--209},
	journaltitle = {Applied Soft Computing},
	shortjournal = {Applied Soft Computing},
	author = {Arar, Ömer Faruk and Ayan, Kürşat},
	urldate = {2021-01-11},
	date = {2017-10-01},
	langid = {english},
	keywords = {Data mining, Discretization, Feature independence, Naive Bayes, Software defect prediction}
}

@article{dulhare_prediction_2018,
	title = {Prediction system for heart disease using Naive Bayes and particle swarm optimization},
	volume = {29},
	doi = {10.4066/biomedicalresearch.29-18-620},
	abstract = {Heart attack disease is major cause of death anywhere in world. Data mining play an important role in health care industry to enable health systems to properly use the data and analytics to identify impotence that improves care with reduce costs. One of data mining technique as classification is a supervised learning used to accurately predict the target class for each case in the data. Heart disease classification involves identifying healthy and sick individuals. Linear classifier as a Naive Bayes ({NB}) is relatively stable with respect to small variation or changes in training data. Particle Swarm Optimization ({PSO}) is an efficient evolutionary computation technique which selects the most optimum features which contribute more to the result which reduces the computation time and increases the accuracy. Experimental result shows that the proposed model with {PSO} as feature selection increases the predictive accuracy of the Naive Bayes to classify heart disease.},
	journaltitle = {Biomedical Research},
	shortjournal = {Biomedical Research},
	author = {Dulhare, Uma},
	date = {2018-01-01}
}

@article{kalutarage_detecting_2015,
	title = {Detecting stealthy attacks: Efficient monitoring of suspicious activities on computer networks},
	volume = {47},
	issn = {0045-7906},
	url = {http://www.sciencedirect.com/science/article/pii/S0045790615002384},
	doi = {10.1016/j.compeleceng.2015.07.007},
	shorttitle = {Detecting stealthy attacks},
	abstract = {Stealthy attackers move patiently through computer networks – taking days, weeks or months to accomplish their objectives in order to avoid detection. As networks scale up in size and speed, monitoring for such attack attempts is increasingly a challenge. This paper presents an efficient monitoring technique for stealthy attacks. It investigates the feasibility of proposed method under number of different test cases and examines how design of the network affects the detection. A methodological way for tracing anonymous stealthy activities to their approximate sources is also presented. The Bayesian fusion along with traffic sampling is employed as a data reduction method. The proposed method has the ability to monitor stealthy activities using 10–20\% size sampling rates without degrading the quality of detection.},
	pages = {327--344},
	journaltitle = {Computers \& Electrical Engineering},
	shortjournal = {Computers \& Electrical Engineering},
	author = {Kalutarage, Harsha K. and Shaikh, Siraj A. and Wickramasinghe, Indika P. and Zhou, Qin and James, Anne E.},
	urldate = {2021-01-11},
	date = {2015-10-01},
	langid = {english},
	keywords = {Anomaly detection, Bayesian fusion, Network simulation, Stealthy attacks, Traffic sampling}
}

@online{noauthor_elements_nodate,
	title = {Elements of Statistical Learning: data mining, inference, and prediction. 2nd Edition.},
	url = {https://web.stanford.edu/~hastie/ElemStatLearn/},
	urldate = {2021-01-12}
}

@book{hastie_elements_2009,
	location = {New York},
	edition = {2},
	title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
	isbn = {978-0-387-84857-0},
	url = {https://www.springer.com/gp/book/9780387848570},
	series = {Springer Series in Statistics},
	shorttitle = {The Elements of Statistical Learning},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-{PLUS} and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including {CART}, {MARS}, projection pursuit and gradient boosting.},
	publisher = {Springer-Verlag},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	urldate = {2021-01-12},
	date = {2009},
	langid = {english},
	doi = {10.1007/978-0-387-84858-7}
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00058655},
	doi = {10.1007/BF00058655},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	pages = {123--140},
	number = {2},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Breiman, Leo},
	urldate = {2021-01-23},
	date = {1996-08-01},
	langid = {english}
}

@article{wolpert_stacked_1992,
	title = {Stacked generalization},
	volume = {5},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608005800231},
	doi = {10.1016/S0893-6080(05)80023-1},
	abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the {NETtalk} task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
	pages = {241--259},
	number = {2},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Wolpert, David H.},
	urldate = {2021-01-23},
	date = {1992-01-01},
	langid = {english},
	keywords = {Combining generalizers, cross-validation, Error estimation and correction, Generalization and induction, Learning set preprocessing}
}